{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0nbAuIuF03JuGK/20vFaL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorDong/Digital-Array/blob/main/Python_Cadence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkA4Owle9Bao",
        "outputId": "059a453e-711a-4e48-c386-6a915802ffed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               -60     -55     -50     -45     -40     -35     -30     -25  \\\n",
            "Vg_Bias(V)                                                                   \n",
            "0.350      -570.55 -510.55 -450.55 -390.65 -330.36 -269.75 -207.27 -137.47   \n",
            "0.375      -515.35 -455.35 -395.35 -335.35 -275.58 -215.97 -156.36  -93.83   \n",
            "0.400      -480.75 -420.75 -360.75 -300.75 -240.63 -180.60 -120.66  -67.98   \n",
            "0.425      -465.40 -405.40 -345.40 -285.40 -225.63 -166.11 -107.78  -60.72   \n",
            "0.450      -452.60 -392.60 -332.60 -272.61 -212.85 -153.47  -96.17  -52.78   \n",
            "0.475      -443.60 -383.60 -323.60 -263.64 -203.91 -144.80  -88.26  -46.64   \n",
            "0.500      -437.45 -377.45 -317.55 -257.70 -198.00 -138.99  -83.36  -42.30   \n",
            "0.525      -433.70 -373.70 -313.70 -253.78 -194.10 -135.34  -80.58  -39.36   \n",
            "0.550      -431.30 -371.30 -311.30 -251.40 -191.75 -133.07  -79.23  -37.60   \n",
            "0.575      -430.05 -370.15 -310.15 -250.19 -190.72 -132.12  -78.94  -36.86   \n",
            "0.600      -429.85 -369.85 -309.85 -250.08 -190.49 -132.09  -79.47  -37.06   \n",
            "0.625      -430.45 -370.45 -310.60 -250.69 -191.26 -132.92  -80.77  -38.11   \n",
            "0.650      -432.10 -372.10 -312.20 -252.40 -192.99 -134.82  -82.93  -39.96   \n",
            "0.675      -434.85 -374.85 -315.10 -255.16 -195.74 -137.83  -86.02  -42.57   \n",
            "0.700      -439.05 -379.05 -319.15 -259.33 -200.01 -142.12  -90.07  -45.94   \n",
            "0.725      -444.50 -384.50 -324.50 -264.71 -205.31 -147.34  -95.04  -50.07   \n",
            "0.750      -450.80 -390.80 -330.80 -270.99 -211.54 -153.58 -100.79  -54.94   \n",
            "0.775      -457.75 -397.75 -337.75 -277.95 -218.68 -160.61 -107.36  -60.59   \n",
            "0.800      -466.25 -406.25 -346.40 -286.63 -227.05 -168.95 -114.98  -67.05   \n",
            "0.825      -476.85 -416.85 -356.85 -296.98 -237.47 -178.85 -123.72  -74.36   \n",
            "0.850      -487.95 -427.95 -367.95 -308.06 -248.37 -189.49 -133.28  -82.36   \n",
            "0.875      -498.00 -438.00 -378.00 -318.25 -258.48 -199.60 -142.88  -90.91   \n",
            "0.900      -507.35 -447.35 -387.35 -327.45 -267.94 -209.11 -152.49  -99.77   \n",
            "0.925      -519.10 -459.10 -399.10 -339.20 -279.39 -220.21 -162.57 -108.90   \n",
            "0.950      -534.50 -474.50 -414.50 -354.50 -294.60 -234.63 -175.22 -118.71   \n",
            "\n",
            "              -20    -19  ...     -9     -8     -7     -6     -5     -4  \\\n",
            "Vg_Bias(V)                ...                                             \n",
            "0.350      -55.51 -43.04  ...  13.34  14.96  16.44  17.82  19.09  20.23   \n",
            "0.375      -39.14 -30.13  ...  14.55  16.07  17.42  18.68  19.79  20.88   \n",
            "0.400      -29.74 -21.17  ...  15.66  17.04  18.30  19.46  20.59  21.49   \n",
            "0.425      -21.38 -13.78  ...  16.70  17.92  19.13  20.28  21.03  21.84   \n",
            "0.450      -14.50  -7.95  ...  17.60  18.80  19.89  20.85  21.78  22.41   \n",
            "0.475       -9.14  -3.36  ...  18.44  19.49  20.49  21.28  22.15  23.09   \n",
            "0.500       -5.11   0.17  ...  19.05  20.24  21.02  21.78  22.73  23.63   \n",
            "0.525       -2.22   2.76  ...  19.71  20.64  21.51  22.28  23.04  23.89   \n",
            "0.550       -0.29   4.56  ...  20.49  21.22  22.05  22.80  23.49  24.27   \n",
            "0.575        0.81   5.70  ...  20.82  21.79  22.53  23.36  23.96  24.67   \n",
            "0.600        1.16   6.22  ...  21.19  22.07  23.00  23.78  24.43  25.08   \n",
            "0.625        0.77   6.11  ...  21.56  22.45  23.38  24.18  24.90  25.50   \n",
            "0.650       -0.29   5.35  ...  21.93  22.76  23.73  24.57  25.32  25.94   \n",
            "0.675       -2.04   3.94  ...  22.30  23.13  24.00  24.87  25.72  26.34   \n",
            "0.700       -4.41   1.94  ...  22.70  23.38  24.20  25.15  26.01  26.74   \n",
            "0.725       -7.39  -0.63  ...  23.10  23.66  24.44  25.30  26.25  27.08   \n",
            "0.750      -10.94  -3.74  ...  23.41  23.96  24.60  25.44  26.39  27.30   \n",
            "0.775      -15.01  -7.32  ...  23.75  24.16  24.77  25.57  26.50  27.48   \n",
            "0.800      -19.56 -11.35  ...  23.98  24.43  24.96  25.76  26.59  27.58   \n",
            "0.825      -24.54 -15.74  ...  24.09  24.59  25.10  25.81  26.69  27.67   \n",
            "0.850      -29.83 -20.38  ...  24.18  24.81  25.27  25.90  26.75  27.72   \n",
            "0.875      -35.41 -25.16  ...  24.24  24.97  25.42  26.02  26.88  27.79   \n",
            "0.900      -41.20 -29.92  ...  24.20  25.08  25.62  26.18  26.89  27.82   \n",
            "0.925      -47.52 -34.66  ...  24.06  25.14  25.76  26.24  26.96  27.91   \n",
            "0.950      -55.68 -40.74  ...  23.91  25.08  25.89  26.37  27.11  28.01   \n",
            "\n",
            "               -3     -2     -1      0  \n",
            "Vg_Bias(V)                              \n",
            "0.350       21.33  23.70  28.54  31.75  \n",
            "0.375       21.84  23.93  28.65  31.77  \n",
            "0.400       22.20  24.22  28.72  31.78  \n",
            "0.425       22.85  24.65  28.81  31.81  \n",
            "0.450       23.37  24.97  28.88  31.83  \n",
            "0.475       23.94  25.25  28.95  31.85  \n",
            "0.500       24.37  25.63  29.04  31.87  \n",
            "0.525       24.98  25.76  29.10  31.88  \n",
            "0.550       25.23  26.17  29.17  31.90  \n",
            "0.575       25.65  26.52  29.24  31.92  \n",
            "0.600       25.97  26.93  29.34  31.94  \n",
            "0.625       26.29  27.30  29.37  31.97  \n",
            "0.650       26.58  27.55  29.48  31.99  \n",
            "0.675       26.94  27.83  29.56  32.00  \n",
            "0.700       27.31  28.13  29.61  32.02  \n",
            "0.725       27.67  28.33  29.70  32.04  \n",
            "0.750       27.99  28.55  29.77  32.06  \n",
            "0.775       28.27  28.81  29.86  32.09  \n",
            "0.800       28.50  29.05  29.96  32.11  \n",
            "0.825       28.67  29.29  30.06  32.13  \n",
            "0.850       28.77  29.50  30.16  32.15  \n",
            "0.875       28.83  29.69  30.27  32.17  \n",
            "0.900       28.87  29.84  30.40  32.20  \n",
            "0.925       28.89  29.95  30.53  32.22  \n",
            "0.950       28.94  30.01  30.65  32.24  \n",
            "\n",
            "[25 rows x 29 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "data = \"\"\"Vg_Bias(V)    -60    -55    -50    -45    -40    -35    -30    -25    -20    -19    -18    -17    -16    -15    -14    -13    -12    -11    -10    -9    -8    -7    -6    -5    -4    -3    -2    -1    0\n",
        "0.350\t-570.55\t-510.55\t-450.55\t-390.65\t-330.36\t-269.75\t-207.27\t-137.47\t-55.51\t-43.04\t-31.95\t-21.65\t-12.23\t-4.61\t0.58\t4.12\t6.92\t9.35\t11.51\t13.34\t14.96\t16.44\t17.82\t19.09\t20.23\t21.33\t23.70\t28.54\t31.75\n",
        "0.375\t-515.35\t-455.35\t-395.35\t-335.35\t-275.58\t-215.97\t-156.36\t-93.83\t-39.14\t-30.13\t-21.10\t-12.55\t-5.44\t-0.26\t3.42\t6.32\t8.84\t11.01\t12.91\t14.55\t16.07\t17.42\t18.68\t19.79\t20.88\t21.84\t23.93\t28.65\t31.77\n",
        "0.400\t-480.75\t-420.75\t-360.75\t-300.75\t-240.63\t-180.60\t-120.66\t-67.98\t-29.74\t-21.17\t-13.09\t-6.26\t-1.00\t2.85\t5.84\t8.36\t10.52\t12.42\t14.10\t15.66\t17.04\t18.30\t19.46\t20.59\t21.49\t22.20\t24.22\t28.72\t31.78\n",
        "0.425\t-465.40\t-405.40\t-345.40\t-285.40\t-225.63\t-166.11\t-107.78\t-60.72\t-21.38\t-13.78\t-7.07\t-1.73\t2.29\t5.34\t7.88\t10.10\t12.01\t13.73\t15.28\t16.70\t17.92\t19.13\t20.28\t21.03\t21.84\t22.85\t24.65\t28.81\t31.81\n",
        "0.450\t-452.60\t-392.60\t-332.60\t-272.61\t-212.85\t-153.47\t-96.17\t-52.78\t-14.50\t-7.95\t-2.47\t1.78\t4.95\t7.47\t9.63\t11.57\t13.33\t14.92\t16.36\t17.60\t18.80\t19.89\t20.85\t21.78\t22.41\t23.37\t24.97\t28.88\t31.83\n",
        "0.475\t-443.60\t-383.60\t-323.60\t-263.64\t-203.91\t-144.80\t-88.26\t-46.64\t-9.14\t-3.36\t1.21\t4.63\t7.26\t9.41\t11.17\t12.86\t14.48\t16.00\t17.32\t18.44\t19.49\t20.49\t21.28\t22.15\t23.09\t23.94\t25.25\t28.95\t31.85\n",
        "0.500\t-437.45\t-377.45\t-317.55\t-257.70\t-198.00\t-138.99\t-83.36\t-42.30\t-5.11\t0.17\t4.13\t7.05\t9.25\t11.09\t12.60\t14.03\t15.47\t17.05\t18.02\t19.05\t20.24\t21.02\t21.78\t22.73\t23.63\t24.37\t25.63\t29.04\t31.87\n",
        "0.525\t-433.70\t-373.70\t-313.70\t-253.78\t-194.10\t-135.34\t-80.58\t-39.36\t-2.22\t2.76\t6.41\t9.06\t11.01\t12.60\t13.95\t15.11\t16.48\t17.61\t18.71\t19.71\t20.64\t21.51\t22.28\t23.04\t23.89\t24.98\t25.76\t29.10\t31.88\n",
        "0.550\t-431.30\t-371.30\t-311.30\t-251.40\t-191.75\t-133.07\t-79.23\t-37.60\t-0.29\t4.56\t8.11\t10.62\t12.48\t14.01\t15.19\t16.16\t17.15\t18.22\t19.34\t20.49\t21.22\t22.05\t22.80\t23.49\t24.27\t25.23\t26.17\t29.17\t31.90\n",
        "0.575\t-430.05\t-370.15\t-310.15\t-250.19\t-190.72\t-132.12\t-78.94\t-36.86\t0.81\t5.70\t9.28\t11.81\t13.64\t15.09\t16.29\t17.19\t17.95\t18.84\t19.92\t20.82\t21.79\t22.53\t23.36\t23.96\t24.67\t25.65\t26.52\t29.24\t31.92\n",
        "0.600\t-429.85\t-369.85\t-309.85\t-250.08\t-190.49\t-132.09\t-79.47\t-37.06\t1.16\t6.22\t9.93\t12.58\t14.49\t15.99\t17.20\t18.13\t18.74\t19.42\t20.25\t21.19\t22.07\t23.00\t23.78\t24.43\t25.08\t25.97\t26.93\t29.34\t31.94\n",
        "0.625\t-430.45\t-370.45\t-310.60\t-250.69\t-191.26\t-132.92\t-80.77\t-38.11\t0.77\t6.11\t10.10\t12.94\t14.99\t16.60\t17.90\t18.88\t19.54\t20.04\t20.79\t21.56\t22.45\t23.38\t24.18\t24.90\t25.50\t26.29\t27.30\t29.37\t31.97\n",
        "0.650\t-432.10\t-372.10\t-312.20\t-252.40\t-192.99\t-134.82\t-82.93\t-39.96\t-0.29\t5.35\t9.72\t12.91\t15.20\t16.93\t18.36\t19.45\t20.21\t20.69\t21.21\t21.93\t22.76\t23.73\t24.57\t25.32\t25.94\t26.58\t27.55\t29.48\t31.99\n",
        "0.675\t-434.85\t-374.85\t-315.10\t-255.16\t-195.74\t-137.83\t-86.02\t-42.57\t-2.04\t3.94\t8.77\t12.42\t15.11\t17.03\t18.58\t19.85\t20.71\t21.27\t21.72\t22.30\t23.13\t24.00\t24.87\t25.72\t26.34\t26.94\t27.83\t29.56\t32.00\n",
        "0.700\t-439.05\t-379.05\t-319.15\t-259.33\t-200.01\t-142.12\t-90.07\t-45.94\t-4.41\t1.94\t7.28\t11.50\t14.64\t16.87\t18.59\t20.03\t21.03\t21.75\t22.21\t22.70\t23.38\t24.20\t25.15\t26.01\t26.74\t27.31\t28.13\t29.61\t32.02\n",
        "0.725\t-444.50\t-384.50\t-324.50\t-264.71\t-205.31\t-147.34\t-95.04\t-50.07\t-7.39\t-0.63\t5.22\t10.07\t13.78\t16.49\t18.40\t19.98\t21.16\t22.06\t22.61\t23.10\t23.66\t24.44\t25.30\t26.25\t27.08\t27.67\t28.33\t29.70\t32.04\n",
        "0.750\t-450.80\t-390.80\t-330.80\t-270.99\t-211.54\t-153.58\t-100.79\t-54.94\t-10.94\t-3.74\t2.65\t8.15\t12.55\t15.78\t18.00\t19.82\t21.24\t22.25\t23.01\t23.41\t23.96\t24.60\t25.44\t26.39\t27.30\t27.99\t28.55\t29.77\t32.06\n",
        "0.775\t-457.75\t-397.75\t-337.75\t-277.95\t-218.68\t-160.61\t-107.36\t-60.59\t-15.01\t-7.32\t-0.40\t5.73\t10.89\t14.74\t17.49\t19.47\t21.13\t22.35\t23.18\t23.75\t24.16\t24.77\t25.57\t26.50\t27.48\t28.27\t28.81\t29.86\t32.09\n",
        "0.800\t-466.25\t-406.25\t-346.40\t-286.63\t-227.05\t-168.95\t-114.98\t-67.05\t-19.56\t-11.35\t-3.85\t2.91\t8.79\t13.43\t16.71\t18.98\t20.94\t22.27\t23.28\t23.98\t24.43\t24.96\t25.76\t26.59\t27.58\t28.50\t29.05\t29.96\t32.11\n",
        "0.825\t-476.85\t-416.85\t-356.85\t-296.98\t-237.47\t-178.85\t-123.72\t-74.36\t-24.54\t-15.74\t-7.65\t-0.27\t6.30\t11.77\t15.67\t18.44\t20.52\t22.17\t23.37\t24.09\t24.59\t25.10\t25.81\t26.69\t27.67\t28.67\t29.29\t30.06\t32.13\n",
        "0.850\t-487.95\t-427.95\t-367.95\t-308.06\t-248.37\t-189.49\t-133.28\t-82.36\t-29.83\t-20.38\t-11.68\t-3.72\t3.53\t9.76\t14.45\t17.65\t19.98\t21.92\t23.40\t24.18\t24.81\t25.27\t25.90\t26.75\t27.72\t28.77\t29.50\t30.16\t32.15\n",
        "0.875\t-498.00\t-438.00\t-378.00\t-318.25\t-258.48\t-199.60\t-142.88\t-90.91\t-35.41\t-25.16\t-15.84\t-7.30\t0.50\t7.43\t12.98\t16.72\t19.29\t21.54\t23.08\t24.24\t24.97\t25.42\t26.02\t26.88\t27.79\t28.83\t29.69\t30.27\t32.17\n",
        "0.900\t-507.35\t-447.35\t-387.35\t-327.45\t-267.94\t-209.11\t-152.49\t-99.77\t-41.20\t-29.92\t-19.83\t-10.82\t-2.57\t4.95\t11.29\t15.72\t18.66\t21.02\t22.83\t24.20\t25.08\t25.62\t26.18\t26.89\t27.82\t28.87\t29.84\t30.40\t32.20\n",
        "0.925\t-519.10\t-459.10\t-399.10\t-339.20\t-279.39\t-220.21\t-162.57\t-108.90\t-47.52\t-34.66\t-23.33\t-13.81\t-5.39\t2.45\t9.42\t14.58\t17.89\t20.44\t22.51\t24.06\t25.14\t25.76\t26.24\t26.96\t27.91\t28.89\t29.95\t30.53\t32.22\n",
        "0.950\t-534.50\t-474.50\t-414.50\t-354.50\t-294.60\t-234.63\t-175.22\t-118.71\t-55.68\t-40.74\t-26.23\t-15.39\t-7.29\t0.34\t7.56\t13.41\t17.13\t19.82\t22.08\t23.91\t25.08\t25.89\t26.37\t27.11\t28.01\t28.94\t30.01\t30.65\t32.24\n",
        "\"\"\"\n",
        "\n",
        "# Rows are Vg Bias (V); Columns are Input Tone Power (dBm)\n",
        "# 10*Pout(dBm) - IMD3(dBm)  = the higher the better\n",
        "\n",
        "# Read the data into a pandas DataFrame\n",
        "data_io = StringIO(data)\n",
        "df = pd.read_csv(data_io, sep='\\s+', index_col=0)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Data Frame to Python Array\n",
        "data = df.values\n",
        "print(data.shape)\n",
        "print(f\"First row:\", data[0])\n",
        "print(f\"First column:\", data[:, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZkdhagdCDqM",
        "outputId": "30d6ef7b-db9e-47c3-bbe9-f2726cd84ab5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 29)\n",
            "First row: [-570.55 -510.55 -450.55 -390.65 -330.36 -269.75 -207.27 -137.47  -55.51\n",
            "  -43.04  -31.95  -21.65  -12.23   -4.61    0.58    4.12    6.92    9.35\n",
            "   11.51   13.34   14.96   16.44   17.82   19.09   20.23   21.33   23.7\n",
            "   28.54   31.75]\n",
            "First column: [-570.55 -515.35 -480.75 -465.4  -452.6  -443.6  -437.45 -433.7  -431.3\n",
            " -430.05 -429.85 -430.45 -432.1  -434.85 -439.05 -444.5  -450.8  -457.75\n",
            " -466.25 -476.85 -487.95 -498.   -507.35 -519.1  -534.5 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Generate input data (Vg_Bias and Input_Tone_Power)\n",
        "vg_bias = np.linspace(0.350, 0.950, 25)\n",
        "# input_tone_power = np.linspace(-60, 0, 29)\n",
        "input_tone_power = [-60.,  -55., -50., -45., -40., -35., -30., -25., -20., -19., -18., -17., -16., -15., -14., -13., -12., -11., -10., -9., -8., -7., -6., -5., -4., -3., -2., -1., 0]\n",
        "X = np.array([[vb, itp] for vb in vg_bias for itp in input_tone_power], dtype=np.float32)\n",
        "y = data.flatten().astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y)\n",
        "\n",
        "# Define the neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)\n",
        "        self.layer2 = nn.Linear(64, 64)\n",
        "        self.output_layer = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "ln3wJgSXBKwA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vg_bias)\n",
        "print(input_tone_power)\n",
        "print(len(input_tone_power))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H52DdX9HC0A",
        "outputId": "df11c2cc-2841-463a-850b-a15cb1668fa6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35  0.375 0.4   0.425 0.45  0.475 0.5   0.525 0.55  0.575 0.6   0.625\n",
            " 0.65  0.675 0.7   0.725 0.75  0.775 0.8   0.825 0.85  0.875 0.9   0.925\n",
            " 0.95 ]\n",
            "[-60.0, -55.0, -50.0, -45.0, -40.0, -35.0, -30.0, -25.0, -20.0, -19.0, -18.0, -17.0, -16.0, -15.0, -14.0, -13.0, -12.0, -11.0, -10.0, -9.0, -8.0, -7.0, -6.0, -5.0, -4.0, -3.0, -2.0, -1.0, 0]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, loss function and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(500000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_tensor).squeeze()\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch [{epoch+1}/500000], Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHhWsT-ZEQ9p",
        "outputId": "08f5bda0-8982-4db7-f266-d627ae352a22"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1000/500000], Loss: 5108.37060546875\n",
            "Epoch [2000/500000], Loss: 2201.531494140625\n",
            "Epoch [3000/500000], Loss: 1670.3209228515625\n",
            "Epoch [4000/500000], Loss: 1453.795654296875\n",
            "Epoch [5000/500000], Loss: 1093.53076171875\n",
            "Epoch [6000/500000], Loss: 757.5501098632812\n",
            "Epoch [7000/500000], Loss: 569.995361328125\n",
            "Epoch [8000/500000], Loss: 500.0724182128906\n",
            "Epoch [9000/500000], Loss: 473.8793640136719\n",
            "Epoch [10000/500000], Loss: 452.6986999511719\n",
            "Epoch [11000/500000], Loss: 430.0211181640625\n",
            "Epoch [12000/500000], Loss: 409.0118103027344\n",
            "Epoch [13000/500000], Loss: 396.7933654785156\n",
            "Epoch [14000/500000], Loss: 388.5552673339844\n",
            "Epoch [15000/500000], Loss: 382.3981018066406\n",
            "Epoch [16000/500000], Loss: 378.2723693847656\n",
            "Epoch [17000/500000], Loss: 373.6117248535156\n",
            "Epoch [18000/500000], Loss: 358.6292724609375\n",
            "Epoch [19000/500000], Loss: 347.71923828125\n",
            "Epoch [20000/500000], Loss: 255.46864318847656\n",
            "Epoch [21000/500000], Loss: 215.06716918945312\n",
            "Epoch [22000/500000], Loss: 161.79212951660156\n",
            "Epoch [23000/500000], Loss: 116.93208312988281\n",
            "Epoch [24000/500000], Loss: 84.86707305908203\n",
            "Epoch [25000/500000], Loss: 65.45604705810547\n",
            "Epoch [26000/500000], Loss: 51.73147201538086\n",
            "Epoch [27000/500000], Loss: 41.27216720581055\n",
            "Epoch [28000/500000], Loss: 33.84858703613281\n",
            "Epoch [29000/500000], Loss: 28.017108917236328\n",
            "Epoch [30000/500000], Loss: 23.679452896118164\n",
            "Epoch [31000/500000], Loss: 20.09626579284668\n",
            "Epoch [32000/500000], Loss: 17.266029357910156\n",
            "Epoch [33000/500000], Loss: 15.087491989135742\n",
            "Epoch [34000/500000], Loss: 13.279411315917969\n",
            "Epoch [35000/500000], Loss: 11.794120788574219\n",
            "Epoch [36000/500000], Loss: 10.41666316986084\n",
            "Epoch [37000/500000], Loss: 9.438633918762207\n",
            "Epoch [38000/500000], Loss: 8.456777572631836\n",
            "Epoch [39000/500000], Loss: 7.5016913414001465\n",
            "Epoch [40000/500000], Loss: 6.676225662231445\n",
            "Epoch [41000/500000], Loss: 6.000854015350342\n",
            "Epoch [42000/500000], Loss: 5.486882209777832\n",
            "Epoch [43000/500000], Loss: 4.764787673950195\n",
            "Epoch [44000/500000], Loss: 4.250984191894531\n",
            "Epoch [45000/500000], Loss: 3.814852476119995\n",
            "Epoch [46000/500000], Loss: 3.5367629528045654\n",
            "Epoch [47000/500000], Loss: 3.206541061401367\n",
            "Epoch [48000/500000], Loss: 2.97981333732605\n",
            "Epoch [49000/500000], Loss: 2.735008478164673\n",
            "Epoch [50000/500000], Loss: 2.593154191970825\n",
            "Epoch [51000/500000], Loss: 2.4015254974365234\n",
            "Epoch [52000/500000], Loss: 2.2704977989196777\n",
            "Epoch [53000/500000], Loss: 2.163883686065674\n",
            "Epoch [54000/500000], Loss: 2.0532476902008057\n",
            "Epoch [55000/500000], Loss: 1.9572744369506836\n",
            "Epoch [56000/500000], Loss: 1.8551981449127197\n",
            "Epoch [57000/500000], Loss: 1.7888224124908447\n",
            "Epoch [58000/500000], Loss: 1.7110155820846558\n",
            "Epoch [59000/500000], Loss: 1.6413214206695557\n",
            "Epoch [60000/500000], Loss: 1.5687534809112549\n",
            "Epoch [61000/500000], Loss: 1.5221211910247803\n",
            "Epoch [62000/500000], Loss: 1.4612666368484497\n",
            "Epoch [63000/500000], Loss: 1.4174679517745972\n",
            "Epoch [64000/500000], Loss: 1.4086655378341675\n",
            "Epoch [65000/500000], Loss: 1.3326807022094727\n",
            "Epoch [66000/500000], Loss: 1.3000129461288452\n",
            "Epoch [67000/500000], Loss: 1.252701759338379\n",
            "Epoch [68000/500000], Loss: 1.2553982734680176\n",
            "Epoch [69000/500000], Loss: 1.1752160787582397\n",
            "Epoch [70000/500000], Loss: 1.1706576347351074\n",
            "Epoch [71000/500000], Loss: 1.1175181865692139\n",
            "Epoch [72000/500000], Loss: 1.0880188941955566\n",
            "Epoch [73000/500000], Loss: 1.0589841604232788\n",
            "Epoch [74000/500000], Loss: 1.034517526626587\n",
            "Epoch [75000/500000], Loss: 1.0120136737823486\n",
            "Epoch [76000/500000], Loss: 0.9911035895347595\n",
            "Epoch [77000/500000], Loss: 0.9733334183692932\n",
            "Epoch [78000/500000], Loss: 0.9595175981521606\n",
            "Epoch [79000/500000], Loss: 0.9336196780204773\n",
            "Epoch [80000/500000], Loss: 0.9374887347221375\n",
            "Epoch [81000/500000], Loss: 0.9013515114784241\n",
            "Epoch [82000/500000], Loss: 0.8864631652832031\n",
            "Epoch [83000/500000], Loss: 0.8729519844055176\n",
            "Epoch [84000/500000], Loss: 0.858482301235199\n",
            "Epoch [85000/500000], Loss: 0.8471189737319946\n",
            "Epoch [86000/500000], Loss: 0.8327242732048035\n",
            "Epoch [87000/500000], Loss: 0.8200511932373047\n",
            "Epoch [88000/500000], Loss: 0.8081991076469421\n",
            "Epoch [89000/500000], Loss: 0.8020684719085693\n",
            "Epoch [90000/500000], Loss: 0.785129964351654\n",
            "Epoch [91000/500000], Loss: 0.7833349704742432\n",
            "Epoch [92000/500000], Loss: 0.7659518718719482\n",
            "Epoch [93000/500000], Loss: 0.755669355392456\n",
            "Epoch [94000/500000], Loss: 0.7464845180511475\n",
            "Epoch [95000/500000], Loss: 0.7381614446640015\n",
            "Epoch [96000/500000], Loss: 0.7318580746650696\n",
            "Epoch [97000/500000], Loss: 0.7215259671211243\n",
            "Epoch [98000/500000], Loss: 0.7175087332725525\n",
            "Epoch [99000/500000], Loss: 0.705680787563324\n",
            "Epoch [100000/500000], Loss: 0.6988517642021179\n",
            "Epoch [101000/500000], Loss: 0.6920210123062134\n",
            "Epoch [102000/500000], Loss: 0.6862939596176147\n",
            "Epoch [103000/500000], Loss: 0.6797243356704712\n",
            "Epoch [104000/500000], Loss: 0.6780750751495361\n",
            "Epoch [105000/500000], Loss: 0.6682605147361755\n",
            "Epoch [106000/500000], Loss: 0.6635158658027649\n",
            "Epoch [107000/500000], Loss: 0.6575568318367004\n",
            "Epoch [108000/500000], Loss: 0.6526936292648315\n",
            "Epoch [109000/500000], Loss: 0.6461934447288513\n",
            "Epoch [110000/500000], Loss: 0.6395755410194397\n",
            "Epoch [111000/500000], Loss: 0.6332778334617615\n",
            "Epoch [112000/500000], Loss: 0.628194272518158\n",
            "Epoch [113000/500000], Loss: 0.6370159983634949\n",
            "Epoch [114000/500000], Loss: 0.6372736096382141\n",
            "Epoch [115000/500000], Loss: 0.6279191970825195\n",
            "Epoch [116000/500000], Loss: 0.611000657081604\n",
            "Epoch [117000/500000], Loss: 0.6071583032608032\n",
            "Epoch [118000/500000], Loss: 0.6250951290130615\n",
            "Epoch [119000/500000], Loss: 0.6564204096794128\n",
            "Epoch [120000/500000], Loss: 0.5959340333938599\n",
            "Epoch [121000/500000], Loss: 0.7460644245147705\n",
            "Epoch [122000/500000], Loss: 0.5894601941108704\n",
            "Epoch [123000/500000], Loss: 0.5861923098564148\n",
            "Epoch [124000/500000], Loss: 0.5829654335975647\n",
            "Epoch [125000/500000], Loss: 0.5797998309135437\n",
            "Epoch [126000/500000], Loss: 0.5834487676620483\n",
            "Epoch [127000/500000], Loss: 0.5793755054473877\n",
            "Epoch [128000/500000], Loss: 0.6507660150527954\n",
            "Epoch [129000/500000], Loss: 0.568249523639679\n",
            "Epoch [130000/500000], Loss: 0.5709043145179749\n",
            "Epoch [131000/500000], Loss: 0.5642992258071899\n",
            "Epoch [132000/500000], Loss: 0.5621896982192993\n",
            "Epoch [133000/500000], Loss: 0.5604554414749146\n",
            "Epoch [134000/500000], Loss: 0.6608958840370178\n",
            "Epoch [135000/500000], Loss: 0.5502755641937256\n",
            "Epoch [136000/500000], Loss: 0.5478611588478088\n",
            "Epoch [137000/500000], Loss: 0.5453481674194336\n",
            "Epoch [138000/500000], Loss: 0.5745949149131775\n",
            "Epoch [139000/500000], Loss: 0.560341477394104\n",
            "Epoch [140000/500000], Loss: 0.5778933763504028\n",
            "Epoch [141000/500000], Loss: 0.5369392037391663\n",
            "Epoch [142000/500000], Loss: 0.5344219207763672\n",
            "Epoch [143000/500000], Loss: 0.5379877090454102\n",
            "Epoch [144000/500000], Loss: 0.5398771166801453\n",
            "Epoch [145000/500000], Loss: 0.5287006497383118\n",
            "Epoch [146000/500000], Loss: 0.5301699042320251\n",
            "Epoch [147000/500000], Loss: 0.525530219078064\n",
            "Epoch [148000/500000], Loss: 0.5357238054275513\n",
            "Epoch [149000/500000], Loss: 0.5367085337638855\n",
            "Epoch [150000/500000], Loss: 0.525057852268219\n",
            "Epoch [151000/500000], Loss: 0.5193852782249451\n",
            "Epoch [152000/500000], Loss: 0.5134167075157166\n",
            "Epoch [153000/500000], Loss: 0.5088575482368469\n",
            "Epoch [154000/500000], Loss: 0.5067099332809448\n",
            "Epoch [155000/500000], Loss: 0.5116767287254333\n",
            "Epoch [156000/500000], Loss: 0.512395977973938\n",
            "Epoch [157000/500000], Loss: 0.4986468255519867\n",
            "Epoch [158000/500000], Loss: 0.5040068626403809\n",
            "Epoch [159000/500000], Loss: 0.49431800842285156\n",
            "Epoch [160000/500000], Loss: 0.4958922863006592\n",
            "Epoch [161000/500000], Loss: 0.49994251132011414\n",
            "Epoch [162000/500000], Loss: 0.49676093459129333\n",
            "Epoch [163000/500000], Loss: 0.486769437789917\n",
            "Epoch [164000/500000], Loss: 0.5231096148490906\n",
            "Epoch [165000/500000], Loss: 0.48360759019851685\n",
            "Epoch [166000/500000], Loss: 0.48314398527145386\n",
            "Epoch [167000/500000], Loss: 0.4829394519329071\n",
            "Epoch [168000/500000], Loss: 0.4790160357952118\n",
            "Epoch [169000/500000], Loss: 0.4775797724723816\n",
            "Epoch [170000/500000], Loss: 0.502439022064209\n",
            "Epoch [171000/500000], Loss: 0.4749267101287842\n",
            "Epoch [172000/500000], Loss: 0.47370705008506775\n",
            "Epoch [173000/500000], Loss: 0.4774778485298157\n",
            "Epoch [174000/500000], Loss: 0.4712693691253662\n",
            "Epoch [175000/500000], Loss: 0.4711190462112427\n",
            "Epoch [176000/500000], Loss: 0.5066454410552979\n",
            "Epoch [177000/500000], Loss: 0.5053128600120544\n",
            "Epoch [178000/500000], Loss: 0.5045873522758484\n",
            "Epoch [179000/500000], Loss: 0.4661409258842468\n",
            "Epoch [180000/500000], Loss: 0.4768710732460022\n",
            "Epoch [181000/500000], Loss: 0.5395123958587646\n",
            "Epoch [182000/500000], Loss: 0.4712252616882324\n",
            "Epoch [183000/500000], Loss: 0.46140554547309875\n",
            "Epoch [184000/500000], Loss: 0.4736405611038208\n",
            "Epoch [185000/500000], Loss: 0.4591342806816101\n",
            "Epoch [186000/500000], Loss: 0.46225494146347046\n",
            "Epoch [187000/500000], Loss: 0.4596775770187378\n",
            "Epoch [188000/500000], Loss: 0.4871026873588562\n",
            "Epoch [189000/500000], Loss: 0.46135810017585754\n",
            "Epoch [190000/500000], Loss: 0.46435168385505676\n",
            "Epoch [191000/500000], Loss: 0.4645700454711914\n",
            "Epoch [192000/500000], Loss: 0.46239450573921204\n",
            "Epoch [193000/500000], Loss: 0.4533662497997284\n",
            "Epoch [194000/500000], Loss: 0.4510383605957031\n",
            "Epoch [195000/500000], Loss: 0.45686835050582886\n",
            "Epoch [196000/500000], Loss: 0.4544909596443176\n",
            "Epoch [197000/500000], Loss: 0.4552996754646301\n",
            "Epoch [198000/500000], Loss: 0.4516298770904541\n",
            "Epoch [199000/500000], Loss: 0.44668546319007874\n",
            "Epoch [200000/500000], Loss: 0.446529358625412\n",
            "Epoch [201000/500000], Loss: 0.44516685605049133\n",
            "Epoch [202000/500000], Loss: 0.4454455077648163\n",
            "Epoch [203000/500000], Loss: 0.44367921352386475\n",
            "Epoch [204000/500000], Loss: 0.44375714659690857\n",
            "Epoch [205000/500000], Loss: 0.5691397786140442\n",
            "Epoch [206000/500000], Loss: 0.46880000829696655\n",
            "Epoch [207000/500000], Loss: 0.44090592861175537\n",
            "Epoch [208000/500000], Loss: 0.4602346420288086\n",
            "Epoch [209000/500000], Loss: 0.4874858260154724\n",
            "Epoch [210000/500000], Loss: 0.43895113468170166\n",
            "Epoch [211000/500000], Loss: 0.4684162735939026\n",
            "Epoch [212000/500000], Loss: 0.4368042051792145\n",
            "Epoch [213000/500000], Loss: 0.4452971816062927\n",
            "Epoch [214000/500000], Loss: 0.43894827365875244\n",
            "Epoch [215000/500000], Loss: 0.43420401215553284\n",
            "Epoch [216000/500000], Loss: 0.43370798230171204\n",
            "Epoch [217000/500000], Loss: 0.43352094292640686\n",
            "Epoch [218000/500000], Loss: 0.433750718832016\n",
            "Epoch [219000/500000], Loss: 0.43805545568466187\n",
            "Epoch [220000/500000], Loss: 0.4407969117164612\n",
            "Epoch [221000/500000], Loss: 0.4317394495010376\n",
            "Epoch [222000/500000], Loss: 0.43067893385887146\n",
            "Epoch [223000/500000], Loss: 0.43012332916259766\n",
            "Epoch [224000/500000], Loss: 0.43278077244758606\n",
            "Epoch [225000/500000], Loss: 0.42770081758499146\n",
            "Epoch [226000/500000], Loss: 0.427643358707428\n",
            "Epoch [227000/500000], Loss: 0.4268125891685486\n",
            "Epoch [228000/500000], Loss: 0.43984243273735046\n",
            "Epoch [229000/500000], Loss: 0.4279526174068451\n",
            "Epoch [230000/500000], Loss: 0.42464280128479004\n",
            "Epoch [231000/500000], Loss: 0.44387051463127136\n",
            "Epoch [232000/500000], Loss: 0.4233691692352295\n",
            "Epoch [233000/500000], Loss: 0.47856903076171875\n",
            "Epoch [234000/500000], Loss: 0.4224870204925537\n",
            "Epoch [235000/500000], Loss: 0.4224773943424225\n",
            "Epoch [236000/500000], Loss: 0.42434927821159363\n",
            "Epoch [237000/500000], Loss: 0.42306816577911377\n",
            "Epoch [238000/500000], Loss: 0.41995200514793396\n",
            "Epoch [239000/500000], Loss: 0.43897587060928345\n",
            "Epoch [240000/500000], Loss: 0.41885367035865784\n",
            "Epoch [241000/500000], Loss: 0.42309924960136414\n",
            "Epoch [242000/500000], Loss: 0.43301665782928467\n",
            "Epoch [243000/500000], Loss: 0.416997492313385\n",
            "Epoch [244000/500000], Loss: 0.4166802763938904\n",
            "Epoch [245000/500000], Loss: 0.41611990332603455\n",
            "Epoch [246000/500000], Loss: 0.4665806293487549\n",
            "Epoch [247000/500000], Loss: 0.41775980591773987\n",
            "Epoch [248000/500000], Loss: 0.4889562427997589\n",
            "Epoch [249000/500000], Loss: 0.4361385405063629\n",
            "Epoch [250000/500000], Loss: 0.41850870847702026\n",
            "Epoch [251000/500000], Loss: 0.41866829991340637\n",
            "Epoch [252000/500000], Loss: 0.42250242829322815\n",
            "Epoch [253000/500000], Loss: 0.4222983419895172\n",
            "Epoch [254000/500000], Loss: 0.43658292293548584\n",
            "Epoch [255000/500000], Loss: 0.5160987973213196\n",
            "Epoch [256000/500000], Loss: 0.4104531705379486\n",
            "Epoch [257000/500000], Loss: 0.418780654668808\n",
            "Epoch [258000/500000], Loss: 0.4417632520198822\n",
            "Epoch [259000/500000], Loss: 0.408940851688385\n",
            "Epoch [260000/500000], Loss: 0.4175190329551697\n",
            "Epoch [261000/500000], Loss: 0.40702396631240845\n",
            "Epoch [262000/500000], Loss: 0.4122845530509949\n",
            "Epoch [263000/500000], Loss: 0.40592801570892334\n",
            "Epoch [264000/500000], Loss: 0.4059147834777832\n",
            "Epoch [265000/500000], Loss: 0.4050268232822418\n",
            "Epoch [266000/500000], Loss: 0.40462377667427063\n",
            "Epoch [267000/500000], Loss: 0.5082802772521973\n",
            "Epoch [268000/500000], Loss: 0.40432700514793396\n",
            "Epoch [269000/500000], Loss: 0.4035576283931732\n",
            "Epoch [270000/500000], Loss: 0.40306127071380615\n",
            "Epoch [271000/500000], Loss: 0.40216583013534546\n",
            "Epoch [272000/500000], Loss: 0.4097118675708771\n",
            "Epoch [273000/500000], Loss: 0.40149861574172974\n",
            "Epoch [274000/500000], Loss: 0.4009861946105957\n",
            "Epoch [275000/500000], Loss: 0.48302143812179565\n",
            "Epoch [276000/500000], Loss: 0.4138159155845642\n",
            "Epoch [277000/500000], Loss: 0.4006321132183075\n",
            "Epoch [278000/500000], Loss: 0.39964404702186584\n",
            "Epoch [279000/500000], Loss: 0.39888259768486023\n",
            "Epoch [280000/500000], Loss: 0.42416879534721375\n",
            "Epoch [281000/500000], Loss: 0.4463582932949066\n",
            "Epoch [282000/500000], Loss: 0.39752665162086487\n",
            "Epoch [283000/500000], Loss: 0.3971099257469177\n",
            "Epoch [284000/500000], Loss: 0.39649128913879395\n",
            "Epoch [285000/500000], Loss: 0.4016816318035126\n",
            "Epoch [286000/500000], Loss: 0.39566925168037415\n",
            "Epoch [287000/500000], Loss: 0.42474833130836487\n",
            "Epoch [288000/500000], Loss: 0.3982917368412018\n",
            "Epoch [289000/500000], Loss: 0.39958688616752625\n",
            "Epoch [290000/500000], Loss: 0.47476184368133545\n",
            "Epoch [291000/500000], Loss: 0.39224812388420105\n",
            "Epoch [292000/500000], Loss: 0.4030051529407501\n",
            "Epoch [293000/500000], Loss: 0.39018481969833374\n",
            "Epoch [294000/500000], Loss: 0.3941926658153534\n",
            "Epoch [295000/500000], Loss: 0.3884580433368683\n",
            "Epoch [296000/500000], Loss: 0.39108172059059143\n",
            "Epoch [297000/500000], Loss: 0.3871959149837494\n",
            "Epoch [298000/500000], Loss: 0.3882169723510742\n",
            "Epoch [299000/500000], Loss: 0.3857528865337372\n",
            "Epoch [300000/500000], Loss: 0.3862271010875702\n",
            "Epoch [301000/500000], Loss: 0.3883626163005829\n",
            "Epoch [302000/500000], Loss: 0.38885098695755005\n",
            "Epoch [303000/500000], Loss: 0.383925199508667\n",
            "Epoch [304000/500000], Loss: 0.3827664852142334\n",
            "Epoch [305000/500000], Loss: 0.38180387020111084\n",
            "Epoch [306000/500000], Loss: 0.3815189003944397\n",
            "Epoch [307000/500000], Loss: 0.38544172048568726\n",
            "Epoch [308000/500000], Loss: 0.4162744879722595\n",
            "Epoch [309000/500000], Loss: 0.3796617090702057\n",
            "Epoch [310000/500000], Loss: 0.38156333565711975\n",
            "Epoch [311000/500000], Loss: 0.38040784001350403\n",
            "Epoch [312000/500000], Loss: 0.3792687654495239\n",
            "Epoch [313000/500000], Loss: 0.379904568195343\n",
            "Epoch [314000/500000], Loss: 0.41939255595207214\n",
            "Epoch [315000/500000], Loss: 0.3793222904205322\n",
            "Epoch [316000/500000], Loss: 0.4206527769565582\n",
            "Epoch [317000/500000], Loss: 0.40446528792381287\n",
            "Epoch [318000/500000], Loss: 0.37599390745162964\n",
            "Epoch [319000/500000], Loss: 0.37994465231895447\n",
            "Epoch [320000/500000], Loss: 0.3774150609970093\n",
            "Epoch [321000/500000], Loss: 0.3736446499824524\n",
            "Epoch [322000/500000], Loss: 0.3772200345993042\n",
            "Epoch [323000/500000], Loss: 0.37470531463623047\n",
            "Epoch [324000/500000], Loss: 0.3815675973892212\n",
            "Epoch [325000/500000], Loss: 0.37577468156814575\n",
            "Epoch [326000/500000], Loss: 0.3755543529987335\n",
            "Epoch [327000/500000], Loss: 0.4043624699115753\n",
            "Epoch [328000/500000], Loss: 0.372615784406662\n",
            "Epoch [329000/500000], Loss: 0.3704986870288849\n",
            "Epoch [330000/500000], Loss: 0.37952637672424316\n",
            "Epoch [331000/500000], Loss: 0.36926379799842834\n",
            "Epoch [332000/500000], Loss: 0.3791152834892273\n",
            "Epoch [333000/500000], Loss: 0.4608311653137207\n",
            "Epoch [334000/500000], Loss: 0.3684599697589874\n",
            "Epoch [335000/500000], Loss: 0.4237648546695709\n",
            "Epoch [336000/500000], Loss: 0.368469774723053\n",
            "Epoch [337000/500000], Loss: 0.38633212447166443\n",
            "Epoch [338000/500000], Loss: 0.41746985912323\n",
            "Epoch [339000/500000], Loss: 0.3677443861961365\n",
            "Epoch [340000/500000], Loss: 0.3688211441040039\n",
            "Epoch [341000/500000], Loss: 0.37069711089134216\n",
            "Epoch [342000/500000], Loss: 0.36698776483535767\n",
            "Epoch [343000/500000], Loss: 0.37861189246177673\n",
            "Epoch [344000/500000], Loss: 0.4124504029750824\n",
            "Epoch [345000/500000], Loss: 0.36417102813720703\n",
            "Epoch [346000/500000], Loss: 0.3645712733268738\n",
            "Epoch [347000/500000], Loss: 0.3893861472606659\n",
            "Epoch [348000/500000], Loss: 0.36316215991973877\n",
            "Epoch [349000/500000], Loss: 0.36245837807655334\n",
            "Epoch [350000/500000], Loss: 0.3622048497200012\n",
            "Epoch [351000/500000], Loss: 0.3617475926876068\n",
            "Epoch [352000/500000], Loss: 0.3682176470756531\n",
            "Epoch [353000/500000], Loss: 0.4239172041416168\n",
            "Epoch [354000/500000], Loss: 0.3610474169254303\n",
            "Epoch [355000/500000], Loss: 0.36494362354278564\n",
            "Epoch [356000/500000], Loss: 0.3606034815311432\n",
            "Epoch [357000/500000], Loss: 0.39059996604919434\n",
            "Epoch [358000/500000], Loss: 0.3593031167984009\n",
            "Epoch [359000/500000], Loss: 0.37542837858200073\n",
            "Epoch [360000/500000], Loss: 0.3594287633895874\n",
            "Epoch [361000/500000], Loss: 0.3796727955341339\n",
            "Epoch [362000/500000], Loss: 0.35488638281822205\n",
            "Epoch [363000/500000], Loss: 0.36492612957954407\n",
            "Epoch [364000/500000], Loss: 0.3578524589538574\n",
            "Epoch [365000/500000], Loss: 0.3523908853530884\n",
            "Epoch [366000/500000], Loss: 0.35223060846328735\n",
            "Epoch [367000/500000], Loss: 0.3510201573371887\n",
            "Epoch [368000/500000], Loss: 0.35055282711982727\n",
            "Epoch [369000/500000], Loss: 0.35402700304985046\n",
            "Epoch [370000/500000], Loss: 0.3499259948730469\n",
            "Epoch [371000/500000], Loss: 0.35001397132873535\n",
            "Epoch [372000/500000], Loss: 0.4383184611797333\n",
            "Epoch [373000/500000], Loss: 0.42438915371894836\n",
            "Epoch [374000/500000], Loss: 0.347563236951828\n",
            "Epoch [375000/500000], Loss: 0.3471629321575165\n",
            "Epoch [376000/500000], Loss: 0.34648385643959045\n",
            "Epoch [377000/500000], Loss: 0.35731053352355957\n",
            "Epoch [378000/500000], Loss: 0.3468172252178192\n",
            "Epoch [379000/500000], Loss: 0.3457939624786377\n",
            "Epoch [380000/500000], Loss: 0.3584580719470978\n",
            "Epoch [381000/500000], Loss: 0.3464171886444092\n",
            "Epoch [382000/500000], Loss: 0.34383657574653625\n",
            "Epoch [383000/500000], Loss: 0.3434387147426605\n",
            "Epoch [384000/500000], Loss: 0.3426711857318878\n",
            "Epoch [385000/500000], Loss: 0.3427092432975769\n",
            "Epoch [386000/500000], Loss: 0.34251105785369873\n",
            "Epoch [387000/500000], Loss: 0.341325968503952\n",
            "Epoch [388000/500000], Loss: 0.3409290909767151\n",
            "Epoch [389000/500000], Loss: 0.3440808057785034\n",
            "Epoch [390000/500000], Loss: 0.34011536836624146\n",
            "Epoch [391000/500000], Loss: 0.33982473611831665\n",
            "Epoch [392000/500000], Loss: 0.3393685221672058\n",
            "Epoch [393000/500000], Loss: 0.3391413688659668\n",
            "Epoch [394000/500000], Loss: 0.34120216965675354\n",
            "Epoch [395000/500000], Loss: 0.33831480145454407\n",
            "Epoch [396000/500000], Loss: 0.33810287714004517\n",
            "Epoch [397000/500000], Loss: 0.3565199077129364\n",
            "Epoch [398000/500000], Loss: 0.3376446068286896\n",
            "Epoch [399000/500000], Loss: 0.33685749769210815\n",
            "Epoch [400000/500000], Loss: 0.35778188705444336\n",
            "Epoch [401000/500000], Loss: 0.33602210879325867\n",
            "Epoch [402000/500000], Loss: 0.33828285336494446\n",
            "Epoch [403000/500000], Loss: 0.35902440547943115\n",
            "Epoch [404000/500000], Loss: 0.3380674719810486\n",
            "Epoch [405000/500000], Loss: 0.35002225637435913\n",
            "Epoch [406000/500000], Loss: 0.35347962379455566\n",
            "Epoch [407000/500000], Loss: 0.3352920413017273\n",
            "Epoch [408000/500000], Loss: 0.33401191234588623\n",
            "Epoch [409000/500000], Loss: 0.33671244978904724\n",
            "Epoch [410000/500000], Loss: 0.33137351274490356\n",
            "Epoch [411000/500000], Loss: 0.33129292726516724\n",
            "Epoch [412000/500000], Loss: 0.3299989402294159\n",
            "Epoch [413000/500000], Loss: 0.3295825719833374\n",
            "Epoch [414000/500000], Loss: 0.3631023168563843\n",
            "Epoch [415000/500000], Loss: 0.3294847309589386\n",
            "Epoch [416000/500000], Loss: 0.3427187204360962\n",
            "Epoch [417000/500000], Loss: 0.32801300287246704\n",
            "Epoch [418000/500000], Loss: 0.34716206789016724\n",
            "Epoch [419000/500000], Loss: 0.3263412117958069\n",
            "Epoch [420000/500000], Loss: 0.3304589092731476\n",
            "Epoch [421000/500000], Loss: 0.3372093141078949\n",
            "Epoch [422000/500000], Loss: 0.3336637020111084\n",
            "Epoch [423000/500000], Loss: 0.3280923664569855\n",
            "Epoch [424000/500000], Loss: 0.3234889805316925\n",
            "Epoch [425000/500000], Loss: 0.3230670690536499\n",
            "Epoch [426000/500000], Loss: 0.3376195728778839\n",
            "Epoch [427000/500000], Loss: 0.3553158640861511\n",
            "Epoch [428000/500000], Loss: 0.43660756945610046\n",
            "Epoch [429000/500000], Loss: 0.33322134613990784\n",
            "Epoch [430000/500000], Loss: 0.3216342329978943\n",
            "Epoch [431000/500000], Loss: 0.32417207956314087\n",
            "Epoch [432000/500000], Loss: 0.323529988527298\n",
            "Epoch [433000/500000], Loss: 0.333962082862854\n",
            "Epoch [434000/500000], Loss: 0.322712779045105\n",
            "Epoch [435000/500000], Loss: 0.32316944003105164\n",
            "Epoch [436000/500000], Loss: 0.3225637674331665\n",
            "Epoch [437000/500000], Loss: 0.3302147388458252\n",
            "Epoch [438000/500000], Loss: 0.38582682609558105\n",
            "Epoch [439000/500000], Loss: 0.3212389647960663\n",
            "Epoch [440000/500000], Loss: 0.317676305770874\n",
            "Epoch [441000/500000], Loss: 0.31760308146476746\n",
            "Epoch [442000/500000], Loss: 0.3331865668296814\n",
            "Epoch [443000/500000], Loss: 0.3175280690193176\n",
            "Epoch [444000/500000], Loss: 0.3251710534095764\n",
            "Epoch [445000/500000], Loss: 0.3164139986038208\n",
            "Epoch [446000/500000], Loss: 0.31627777218818665\n",
            "Epoch [447000/500000], Loss: 0.31569162011146545\n",
            "Epoch [448000/500000], Loss: 0.3216393291950226\n",
            "Epoch [449000/500000], Loss: 0.3185872733592987\n",
            "Epoch [450000/500000], Loss: 0.3198590874671936\n",
            "Epoch [451000/500000], Loss: 0.32581645250320435\n",
            "Epoch [452000/500000], Loss: 0.3141396641731262\n",
            "Epoch [453000/500000], Loss: 0.32323789596557617\n",
            "Epoch [454000/500000], Loss: 0.33078300952911377\n",
            "Epoch [455000/500000], Loss: 0.31342703104019165\n",
            "Epoch [456000/500000], Loss: 0.3166458010673523\n",
            "Epoch [457000/500000], Loss: 0.3157896399497986\n",
            "Epoch [458000/500000], Loss: 0.31581833958625793\n",
            "Epoch [459000/500000], Loss: 0.3128964602947235\n",
            "Epoch [460000/500000], Loss: 0.31722351908683777\n",
            "Epoch [461000/500000], Loss: 0.31209373474121094\n",
            "Epoch [462000/500000], Loss: 0.31280815601348877\n",
            "Epoch [463000/500000], Loss: 0.3296974301338196\n",
            "Epoch [464000/500000], Loss: 0.3347397446632385\n",
            "Epoch [465000/500000], Loss: 0.31644096970558167\n",
            "Epoch [466000/500000], Loss: 0.39100468158721924\n",
            "Epoch [467000/500000], Loss: 0.311310350894928\n",
            "Epoch [468000/500000], Loss: 0.3621598482131958\n",
            "Epoch [469000/500000], Loss: 0.3180721402168274\n",
            "Epoch [470000/500000], Loss: 0.31367868185043335\n",
            "Epoch [471000/500000], Loss: 0.3128996789455414\n",
            "Epoch [472000/500000], Loss: 0.31178221106529236\n",
            "Epoch [473000/500000], Loss: 0.3100227117538452\n",
            "Epoch [474000/500000], Loss: 0.3285810053348541\n",
            "Epoch [475000/500000], Loss: 0.3137783706188202\n",
            "Epoch [476000/500000], Loss: 0.3090420067310333\n",
            "Epoch [477000/500000], Loss: 0.3087642788887024\n",
            "Epoch [478000/500000], Loss: 0.3092402219772339\n",
            "Epoch [479000/500000], Loss: 0.3093195855617523\n",
            "Epoch [480000/500000], Loss: 0.3092304766178131\n",
            "Epoch [481000/500000], Loss: 0.30805596709251404\n",
            "Epoch [482000/500000], Loss: 0.3151470422744751\n",
            "Epoch [483000/500000], Loss: 0.31043604016304016\n",
            "Epoch [484000/500000], Loss: 0.37509685754776\n",
            "Epoch [485000/500000], Loss: 0.3702039122581482\n",
            "Epoch [486000/500000], Loss: 0.30719271302223206\n",
            "Epoch [487000/500000], Loss: 0.30697211623191833\n",
            "Epoch [488000/500000], Loss: 0.3078569173812866\n",
            "Epoch [489000/500000], Loss: 0.356974333524704\n",
            "Epoch [490000/500000], Loss: 0.3065311014652252\n",
            "Epoch [491000/500000], Loss: 0.3339831829071045\n",
            "Epoch [492000/500000], Loss: 0.306389719247818\n",
            "Epoch [493000/500000], Loss: 0.30673399567604065\n",
            "Epoch [494000/500000], Loss: 0.34958934783935547\n",
            "Epoch [495000/500000], Loss: 0.31309154629707336\n",
            "Epoch [496000/500000], Loss: 0.3084069490432739\n",
            "Epoch [497000/500000], Loss: 0.3052760064601898\n",
            "Epoch [498000/500000], Loss: 0.306282103061676\n",
            "Epoch [499000/500000], Loss: 0.30574166774749756\n",
            "Epoch [500000/500000], Loss: 0.3069096803665161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions\n",
        "new_input = torch.tensor([[0.400, -48]], dtype=torch.float32)\n",
        "\n",
        "predicted_imd3_distortion = model(new_input).item()\n",
        "print(f\"Predicted IMD3 Distortion: {predicted_imd3_distortion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zLb8QoQEKCb",
        "outputId": "5db6fcf3-64db-4d4b-c3ec-881c3e9e0f24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted IMD3 Distortion: -337.5373840332031\n"
          ]
        }
      ]
    }
  ]
}