{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvbs8zgFrjI5yLU7ZiUghH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ProfessorDong/Digital-Array/blob/main/Python_Cadence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkA4Owle9Bao",
        "outputId": "dc86873b-1220-40c8-c28d-2963d8f51d00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               -60     -55     -50     -45     -40     -35     -30     -25  \\\n",
            "Vg_Bias(V)                                                                   \n",
            "0.350      -570.55 -510.55 -450.55 -390.65 -330.36 -269.75 -207.27 -137.47   \n",
            "0.375      -515.35 -455.35 -395.35 -335.35 -275.58 -215.97 -156.36  -93.83   \n",
            "0.400      -480.75 -420.75 -360.75 -300.75 -240.63 -180.60 -120.66  -67.98   \n",
            "0.425      -465.40 -405.40 -345.40 -285.40 -225.63 -166.11 -107.78  -60.72   \n",
            "0.450      -452.60 -392.60 -332.60 -272.61 -212.85 -153.47  -96.17  -52.78   \n",
            "0.475      -443.60 -383.60 -323.60 -263.64 -203.91 -144.80  -88.26  -46.64   \n",
            "0.500      -437.45 -377.45 -317.55 -257.70 -198.00 -138.99  -83.36  -42.30   \n",
            "0.525      -433.70 -373.70 -313.70 -253.78 -194.10 -135.34  -80.58  -39.36   \n",
            "0.550      -431.30 -371.30 -311.30 -251.40 -191.75 -133.07  -79.23  -37.60   \n",
            "0.575      -430.05 -370.15 -310.15 -250.19 -190.72 -132.12  -78.94  -36.86   \n",
            "0.600      -429.85 -369.85 -309.85 -250.08 -190.49 -132.09  -79.47  -37.06   \n",
            "0.625      -430.45 -370.45 -310.60 -250.69 -191.26 -132.92  -80.77  -38.11   \n",
            "0.650      -432.10 -372.10 -312.20 -252.40 -192.99 -134.82  -82.93  -39.96   \n",
            "0.675      -434.85 -374.85 -315.10 -255.16 -195.74 -137.83  -86.02  -42.57   \n",
            "0.700      -439.05 -379.05 -319.15 -259.33 -200.01 -142.12  -90.07  -45.94   \n",
            "0.725      -444.50 -384.50 -324.50 -264.71 -205.31 -147.34  -95.04  -50.07   \n",
            "0.750      -450.80 -390.80 -330.80 -270.99 -211.54 -153.58 -100.79  -54.94   \n",
            "0.775      -457.75 -397.75 -337.75 -277.95 -218.68 -160.61 -107.36  -60.59   \n",
            "0.800      -466.25 -406.25 -346.40 -286.63 -227.05 -168.95 -114.98  -67.05   \n",
            "0.825      -476.85 -416.85 -356.85 -296.98 -237.47 -178.85 -123.72  -74.36   \n",
            "0.850      -487.95 -427.95 -367.95 -308.06 -248.37 -189.49 -133.28  -82.36   \n",
            "0.875      -498.00 -438.00 -378.00 -318.25 -258.48 -199.60 -142.88  -90.91   \n",
            "0.900      -507.35 -447.35 -387.35 -327.45 -267.94 -209.11 -152.49  -99.77   \n",
            "0.925      -519.10 -459.10 -399.10 -339.20 -279.39 -220.21 -162.57 -108.90   \n",
            "0.950      -534.50 -474.50 -414.50 -354.50 -294.60 -234.63 -175.22 -118.71   \n",
            "\n",
            "              -20    -19  ...     -9     -8     -7     -6     -5     -4  \\\n",
            "Vg_Bias(V)                ...                                             \n",
            "0.350      -55.51 -43.04  ...  13.34  14.96  16.44  17.82  19.09  20.23   \n",
            "0.375      -39.14 -30.13  ...  14.55  16.07  17.42  18.68  19.79  20.88   \n",
            "0.400      -29.74 -21.17  ...  15.66  17.04  18.30  19.46  20.59  21.49   \n",
            "0.425      -21.38 -13.78  ...  16.70  17.92  19.13  20.28  21.03  21.84   \n",
            "0.450      -14.50  -7.95  ...  17.60  18.80  19.89  20.85  21.78  22.41   \n",
            "0.475       -9.14  -3.36  ...  18.44  19.49  20.49  21.28  22.15  23.09   \n",
            "0.500       -5.11   0.17  ...  19.05  20.24  21.02  21.78  22.73  23.63   \n",
            "0.525       -2.22   2.76  ...  19.71  20.64  21.51  22.28  23.04  23.89   \n",
            "0.550       -0.29   4.56  ...  20.49  21.22  22.05  22.80  23.49  24.27   \n",
            "0.575        0.81   5.70  ...  20.82  21.79  22.53  23.36  23.96  24.67   \n",
            "0.600        1.16   6.22  ...  21.19  22.07  23.00  23.78  24.43  25.08   \n",
            "0.625        0.77   6.11  ...  21.56  22.45  23.38  24.18  24.90  25.50   \n",
            "0.650       -0.29   5.35  ...  21.93  22.76  23.73  24.57  25.32  25.94   \n",
            "0.675       -2.04   3.94  ...  22.30  23.13  24.00  24.87  25.72  26.34   \n",
            "0.700       -4.41   1.94  ...  22.70  23.38  24.20  25.15  26.01  26.74   \n",
            "0.725       -7.39  -0.63  ...  23.10  23.66  24.44  25.30  26.25  27.08   \n",
            "0.750      -10.94  -3.74  ...  23.41  23.96  24.60  25.44  26.39  27.30   \n",
            "0.775      -15.01  -7.32  ...  23.75  24.16  24.77  25.57  26.50  27.48   \n",
            "0.800      -19.56 -11.35  ...  23.98  24.43  24.96  25.76  26.59  27.58   \n",
            "0.825      -24.54 -15.74  ...  24.09  24.59  25.10  25.81  26.69  27.67   \n",
            "0.850      -29.83 -20.38  ...  24.18  24.81  25.27  25.90  26.75  27.72   \n",
            "0.875      -35.41 -25.16  ...  24.24  24.97  25.42  26.02  26.88  27.79   \n",
            "0.900      -41.20 -29.92  ...  24.20  25.08  25.62  26.18  26.89  27.82   \n",
            "0.925      -47.52 -34.66  ...  24.06  25.14  25.76  26.24  26.96  27.91   \n",
            "0.950      -55.68 -40.74  ...  23.91  25.08  25.89  26.37  27.11  28.01   \n",
            "\n",
            "               -3     -2     -1      0  \n",
            "Vg_Bias(V)                              \n",
            "0.350       21.33  23.70  28.54  31.75  \n",
            "0.375       21.84  23.93  28.65  31.77  \n",
            "0.400       22.20  24.22  28.72  31.78  \n",
            "0.425       22.85  24.65  28.81  31.81  \n",
            "0.450       23.37  24.97  28.88  31.83  \n",
            "0.475       23.94  25.25  28.95  31.85  \n",
            "0.500       24.37  25.63  29.04  31.87  \n",
            "0.525       24.98  25.76  29.10  31.88  \n",
            "0.550       25.23  26.17  29.17  31.90  \n",
            "0.575       25.65  26.52  29.24  31.92  \n",
            "0.600       25.97  26.93  29.34  31.94  \n",
            "0.625       26.29  27.30  29.37  31.97  \n",
            "0.650       26.58  27.55  29.48  31.99  \n",
            "0.675       26.94  27.83  29.56  32.00  \n",
            "0.700       27.31  28.13  29.61  32.02  \n",
            "0.725       27.67  28.33  29.70  32.04  \n",
            "0.750       27.99  28.55  29.77  32.06  \n",
            "0.775       28.27  28.81  29.86  32.09  \n",
            "0.800       28.50  29.05  29.96  32.11  \n",
            "0.825       28.67  29.29  30.06  32.13  \n",
            "0.850       28.77  29.50  30.16  32.15  \n",
            "0.875       28.83  29.69  30.27  32.17  \n",
            "0.900       28.87  29.84  30.40  32.20  \n",
            "0.925       28.89  29.95  30.53  32.22  \n",
            "0.950       28.94  30.01  30.65  32.24  \n",
            "\n",
            "[25 rows x 29 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from io import StringIO\n",
        "\n",
        "data = \"\"\"Vg_Bias(V)    -60    -55    -50    -45    -40    -35    -30    -25    -20    -19    -18    -17    -16    -15    -14    -13    -12    -11    -10    -9    -8    -7    -6    -5    -4    -3    -2    -1    0\n",
        "0.350\t-570.55\t-510.55\t-450.55\t-390.65\t-330.36\t-269.75\t-207.27\t-137.47\t-55.51\t-43.04\t-31.95\t-21.65\t-12.23\t-4.61\t0.58\t4.12\t6.92\t9.35\t11.51\t13.34\t14.96\t16.44\t17.82\t19.09\t20.23\t21.33\t23.70\t28.54\t31.75\n",
        "0.375\t-515.35\t-455.35\t-395.35\t-335.35\t-275.58\t-215.97\t-156.36\t-93.83\t-39.14\t-30.13\t-21.10\t-12.55\t-5.44\t-0.26\t3.42\t6.32\t8.84\t11.01\t12.91\t14.55\t16.07\t17.42\t18.68\t19.79\t20.88\t21.84\t23.93\t28.65\t31.77\n",
        "0.400\t-480.75\t-420.75\t-360.75\t-300.75\t-240.63\t-180.60\t-120.66\t-67.98\t-29.74\t-21.17\t-13.09\t-6.26\t-1.00\t2.85\t5.84\t8.36\t10.52\t12.42\t14.10\t15.66\t17.04\t18.30\t19.46\t20.59\t21.49\t22.20\t24.22\t28.72\t31.78\n",
        "0.425\t-465.40\t-405.40\t-345.40\t-285.40\t-225.63\t-166.11\t-107.78\t-60.72\t-21.38\t-13.78\t-7.07\t-1.73\t2.29\t5.34\t7.88\t10.10\t12.01\t13.73\t15.28\t16.70\t17.92\t19.13\t20.28\t21.03\t21.84\t22.85\t24.65\t28.81\t31.81\n",
        "0.450\t-452.60\t-392.60\t-332.60\t-272.61\t-212.85\t-153.47\t-96.17\t-52.78\t-14.50\t-7.95\t-2.47\t1.78\t4.95\t7.47\t9.63\t11.57\t13.33\t14.92\t16.36\t17.60\t18.80\t19.89\t20.85\t21.78\t22.41\t23.37\t24.97\t28.88\t31.83\n",
        "0.475\t-443.60\t-383.60\t-323.60\t-263.64\t-203.91\t-144.80\t-88.26\t-46.64\t-9.14\t-3.36\t1.21\t4.63\t7.26\t9.41\t11.17\t12.86\t14.48\t16.00\t17.32\t18.44\t19.49\t20.49\t21.28\t22.15\t23.09\t23.94\t25.25\t28.95\t31.85\n",
        "0.500\t-437.45\t-377.45\t-317.55\t-257.70\t-198.00\t-138.99\t-83.36\t-42.30\t-5.11\t0.17\t4.13\t7.05\t9.25\t11.09\t12.60\t14.03\t15.47\t17.05\t18.02\t19.05\t20.24\t21.02\t21.78\t22.73\t23.63\t24.37\t25.63\t29.04\t31.87\n",
        "0.525\t-433.70\t-373.70\t-313.70\t-253.78\t-194.10\t-135.34\t-80.58\t-39.36\t-2.22\t2.76\t6.41\t9.06\t11.01\t12.60\t13.95\t15.11\t16.48\t17.61\t18.71\t19.71\t20.64\t21.51\t22.28\t23.04\t23.89\t24.98\t25.76\t29.10\t31.88\n",
        "0.550\t-431.30\t-371.30\t-311.30\t-251.40\t-191.75\t-133.07\t-79.23\t-37.60\t-0.29\t4.56\t8.11\t10.62\t12.48\t14.01\t15.19\t16.16\t17.15\t18.22\t19.34\t20.49\t21.22\t22.05\t22.80\t23.49\t24.27\t25.23\t26.17\t29.17\t31.90\n",
        "0.575\t-430.05\t-370.15\t-310.15\t-250.19\t-190.72\t-132.12\t-78.94\t-36.86\t0.81\t5.70\t9.28\t11.81\t13.64\t15.09\t16.29\t17.19\t17.95\t18.84\t19.92\t20.82\t21.79\t22.53\t23.36\t23.96\t24.67\t25.65\t26.52\t29.24\t31.92\n",
        "0.600\t-429.85\t-369.85\t-309.85\t-250.08\t-190.49\t-132.09\t-79.47\t-37.06\t1.16\t6.22\t9.93\t12.58\t14.49\t15.99\t17.20\t18.13\t18.74\t19.42\t20.25\t21.19\t22.07\t23.00\t23.78\t24.43\t25.08\t25.97\t26.93\t29.34\t31.94\n",
        "0.625\t-430.45\t-370.45\t-310.60\t-250.69\t-191.26\t-132.92\t-80.77\t-38.11\t0.77\t6.11\t10.10\t12.94\t14.99\t16.60\t17.90\t18.88\t19.54\t20.04\t20.79\t21.56\t22.45\t23.38\t24.18\t24.90\t25.50\t26.29\t27.30\t29.37\t31.97\n",
        "0.650\t-432.10\t-372.10\t-312.20\t-252.40\t-192.99\t-134.82\t-82.93\t-39.96\t-0.29\t5.35\t9.72\t12.91\t15.20\t16.93\t18.36\t19.45\t20.21\t20.69\t21.21\t21.93\t22.76\t23.73\t24.57\t25.32\t25.94\t26.58\t27.55\t29.48\t31.99\n",
        "0.675\t-434.85\t-374.85\t-315.10\t-255.16\t-195.74\t-137.83\t-86.02\t-42.57\t-2.04\t3.94\t8.77\t12.42\t15.11\t17.03\t18.58\t19.85\t20.71\t21.27\t21.72\t22.30\t23.13\t24.00\t24.87\t25.72\t26.34\t26.94\t27.83\t29.56\t32.00\n",
        "0.700\t-439.05\t-379.05\t-319.15\t-259.33\t-200.01\t-142.12\t-90.07\t-45.94\t-4.41\t1.94\t7.28\t11.50\t14.64\t16.87\t18.59\t20.03\t21.03\t21.75\t22.21\t22.70\t23.38\t24.20\t25.15\t26.01\t26.74\t27.31\t28.13\t29.61\t32.02\n",
        "0.725\t-444.50\t-384.50\t-324.50\t-264.71\t-205.31\t-147.34\t-95.04\t-50.07\t-7.39\t-0.63\t5.22\t10.07\t13.78\t16.49\t18.40\t19.98\t21.16\t22.06\t22.61\t23.10\t23.66\t24.44\t25.30\t26.25\t27.08\t27.67\t28.33\t29.70\t32.04\n",
        "0.750\t-450.80\t-390.80\t-330.80\t-270.99\t-211.54\t-153.58\t-100.79\t-54.94\t-10.94\t-3.74\t2.65\t8.15\t12.55\t15.78\t18.00\t19.82\t21.24\t22.25\t23.01\t23.41\t23.96\t24.60\t25.44\t26.39\t27.30\t27.99\t28.55\t29.77\t32.06\n",
        "0.775\t-457.75\t-397.75\t-337.75\t-277.95\t-218.68\t-160.61\t-107.36\t-60.59\t-15.01\t-7.32\t-0.40\t5.73\t10.89\t14.74\t17.49\t19.47\t21.13\t22.35\t23.18\t23.75\t24.16\t24.77\t25.57\t26.50\t27.48\t28.27\t28.81\t29.86\t32.09\n",
        "0.800\t-466.25\t-406.25\t-346.40\t-286.63\t-227.05\t-168.95\t-114.98\t-67.05\t-19.56\t-11.35\t-3.85\t2.91\t8.79\t13.43\t16.71\t18.98\t20.94\t22.27\t23.28\t23.98\t24.43\t24.96\t25.76\t26.59\t27.58\t28.50\t29.05\t29.96\t32.11\n",
        "0.825\t-476.85\t-416.85\t-356.85\t-296.98\t-237.47\t-178.85\t-123.72\t-74.36\t-24.54\t-15.74\t-7.65\t-0.27\t6.30\t11.77\t15.67\t18.44\t20.52\t22.17\t23.37\t24.09\t24.59\t25.10\t25.81\t26.69\t27.67\t28.67\t29.29\t30.06\t32.13\n",
        "0.850\t-487.95\t-427.95\t-367.95\t-308.06\t-248.37\t-189.49\t-133.28\t-82.36\t-29.83\t-20.38\t-11.68\t-3.72\t3.53\t9.76\t14.45\t17.65\t19.98\t21.92\t23.40\t24.18\t24.81\t25.27\t25.90\t26.75\t27.72\t28.77\t29.50\t30.16\t32.15\n",
        "0.875\t-498.00\t-438.00\t-378.00\t-318.25\t-258.48\t-199.60\t-142.88\t-90.91\t-35.41\t-25.16\t-15.84\t-7.30\t0.50\t7.43\t12.98\t16.72\t19.29\t21.54\t23.08\t24.24\t24.97\t25.42\t26.02\t26.88\t27.79\t28.83\t29.69\t30.27\t32.17\n",
        "0.900\t-507.35\t-447.35\t-387.35\t-327.45\t-267.94\t-209.11\t-152.49\t-99.77\t-41.20\t-29.92\t-19.83\t-10.82\t-2.57\t4.95\t11.29\t15.72\t18.66\t21.02\t22.83\t24.20\t25.08\t25.62\t26.18\t26.89\t27.82\t28.87\t29.84\t30.40\t32.20\n",
        "0.925\t-519.10\t-459.10\t-399.10\t-339.20\t-279.39\t-220.21\t-162.57\t-108.90\t-47.52\t-34.66\t-23.33\t-13.81\t-5.39\t2.45\t9.42\t14.58\t17.89\t20.44\t22.51\t24.06\t25.14\t25.76\t26.24\t26.96\t27.91\t28.89\t29.95\t30.53\t32.22\n",
        "0.950\t-534.50\t-474.50\t-414.50\t-354.50\t-294.60\t-234.63\t-175.22\t-118.71\t-55.68\t-40.74\t-26.23\t-15.39\t-7.29\t0.34\t7.56\t13.41\t17.13\t19.82\t22.08\t23.91\t25.08\t25.89\t26.37\t27.11\t28.01\t28.94\t30.01\t30.65\t32.24\n",
        "\"\"\"\n",
        "\n",
        "# Rows are Vg Bias (V); Columns are Input Tone Power (dBm)\n",
        "# 10*Pout(dBm) - IMD3(dBm)  = the higher the better\n",
        "\n",
        "# Read the data into a pandas DataFrame\n",
        "data_io = StringIO(data)\n",
        "df = pd.read_csv(data_io, sep='\\s+', index_col=0)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert Data Frame to Python Array\n",
        "data = df.values\n",
        "print(data.shape)\n",
        "print(f\"First row:\", data[0])\n",
        "print(f\"First column:\", data[:, 0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZkdhagdCDqM",
        "outputId": "455f7709-4b21-457e-c7b4-e5624cff704b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(25, 29)\n",
            "First row: [-570.55 -510.55 -450.55 -390.65 -330.36 -269.75 -207.27 -137.47  -55.51\n",
            "  -43.04  -31.95  -21.65  -12.23   -4.61    0.58    4.12    6.92    9.35\n",
            "   11.51   13.34   14.96   16.44   17.82   19.09   20.23   21.33   23.7\n",
            "   28.54   31.75]\n",
            "First column: [-570.55 -515.35 -480.75 -465.4  -452.6  -443.6  -437.45 -433.7  -431.3\n",
            " -430.05 -429.85 -430.45 -432.1  -434.85 -439.05 -444.5  -450.8  -457.75\n",
            " -466.25 -476.85 -487.95 -498.   -507.35 -519.1  -534.5 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "# Generate input data (Vg_Bias and Input_Tone_Power)\n",
        "vg_bias = np.linspace(0.350, 0.950, 25)\n",
        "# input_tone_power = np.linspace(-60, 0, 29)\n",
        "input_tone_power = [-60.,  -55., -50., -45., -40., -35., -30., -25., -20., -19., -18., -17., -16., -15., -14., -13., -12., -11., -10., -9., -8., -7., -6., -5., -4., -3., -2., -1., 0]\n",
        "X = np.array([[vb, itp] for vb in vg_bias for itp in input_tone_power], dtype=np.float32)\n",
        "y = data.flatten().astype(np.float32)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "X_tensor = torch.from_numpy(X)\n",
        "y_tensor = torch.from_numpy(y)\n",
        "\n",
        "display(X_tensor)\n",
        "print(\"Shape of X_tensor: \", X_tensor.shape)\n",
        "print(\"Shape of y_tensor: \", y_tensor.shape)"
      ],
      "metadata": {
        "id": "ln3wJgSXBKwA",
        "outputId": "70b78609-0485-45aa-ea6d-e8ae7c1dfb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[  0.3500, -60.0000],\n",
              "        [  0.3500, -55.0000],\n",
              "        [  0.3500, -50.0000],\n",
              "        ...,\n",
              "        [  0.9500,  -2.0000],\n",
              "        [  0.9500,  -1.0000],\n",
              "        [  0.9500,   0.0000]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_tensor:  torch.Size([725, 2])\n",
            "Shape of y_tensor:  torch.Size([725])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New data points\n",
        "new_X = np.array([[0.7, -14.], [0.8, -14.]], dtype=np.float32)\n",
        "new_y= np.array([16.69, 16.4], dtype=np.float32)\n",
        "new_X_tensor = torch.from_numpy(new_X)\n",
        "new_y_tensor = torch.from_numpy(new_y)\n",
        "\n",
        "display(new_X_tensor)\n",
        "print(\"Shape of new_X_tensor: \", new_X_tensor.shape)\n",
        "print(\"Shape of new_y_tensor: \", new_y_tensor.shape)\n",
        "\n",
        "# Append new data\n",
        "X_tensor = torch.cat((X_tensor, new_X_tensor), dim=0)\n",
        "y_tensor = torch.cat((y_tensor, new_y_tensor), dim=0)\n",
        "\n",
        "display(X_tensor)\n",
        "print(\"Shape of X_tensor: \", X_tensor.shape)\n",
        "# display(y_tensor)\n",
        "print(\"Shape of y_tensor: \", y_tensor.shape)"
      ],
      "metadata": {
        "id": "16-h9fasOzmu",
        "outputId": "0c35070a-7994-4f3f-d2fe-73563764aaf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[  0.7000, -14.0000],\n",
              "        [  0.8000, -14.0000]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of new_X_tensor:  torch.Size([2, 2])\n",
            "Shape of new_y_tensor:  torch.Size([2])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tensor([[  0.3500, -60.0000],\n",
              "        [  0.3500, -55.0000],\n",
              "        [  0.3500, -50.0000],\n",
              "        ...,\n",
              "        [  0.9500,   0.0000],\n",
              "        [  0.7000, -14.0000],\n",
              "        [  0.8000, -14.0000]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_tensor:  torch.Size([727, 2])\n",
            "Shape of y_tensor:  torch.Size([727])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)\n",
        "        self.layer2 = nn.Linear(64, 64)\n",
        "        self.output_layer = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "OdFBgvOJO3n1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vg_bias)\n",
        "print(input_tone_power)\n",
        "print(len(input_tone_power))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9H52DdX9HC0A",
        "outputId": "438f56ca-b749-45c1-a256-3e39120df44c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.35  0.375 0.4   0.425 0.45  0.475 0.5   0.525 0.55  0.575 0.6   0.625\n",
            " 0.65  0.675 0.7   0.725 0.75  0.775 0.8   0.825 0.85  0.875 0.9   0.925\n",
            " 0.95 ]\n",
            "[-60.0, -55.0, -50.0, -45.0, -40.0, -35.0, -30.0, -25.0, -20.0, -19.0, -18.0, -17.0, -16.0, -15.0, -14.0, -13.0, -12.0, -11.0, -10.0, -9.0, -8.0, -7.0, -6.0, -5.0, -4.0, -3.0, -2.0, -1.0, 0]\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model, loss function and optimizer\n",
        "model = NeuralNetwork()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training the model\n",
        "for epoch in range(500000):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(X_tensor).squeeze()\n",
        "    loss = criterion(outputs, y_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        print(f'Epoch [{epoch+1}/500000], Loss: {loss.item()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHhWsT-ZEQ9p",
        "outputId": "631965bf-ae1d-40f1-bd78-d91f6a5b2417"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1000/500000], Loss: 7152.9658203125\n",
            "Epoch [2000/500000], Loss: 2375.41015625\n",
            "Epoch [3000/500000], Loss: 1816.3060302734375\n",
            "Epoch [4000/500000], Loss: 1463.5670166015625\n",
            "Epoch [5000/500000], Loss: 1029.4315185546875\n",
            "Epoch [6000/500000], Loss: 715.3193359375\n",
            "Epoch [7000/500000], Loss: 558.8568115234375\n",
            "Epoch [8000/500000], Loss: 502.9337463378906\n",
            "Epoch [9000/500000], Loss: 476.6367492675781\n",
            "Epoch [10000/500000], Loss: 448.62664794921875\n",
            "Epoch [11000/500000], Loss: 424.65960693359375\n",
            "Epoch [12000/500000], Loss: 407.1854248046875\n",
            "Epoch [13000/500000], Loss: 394.29547119140625\n",
            "Epoch [14000/500000], Loss: 385.7594909667969\n",
            "Epoch [15000/500000], Loss: 380.3376159667969\n",
            "Epoch [16000/500000], Loss: 373.8608093261719\n",
            "Epoch [17000/500000], Loss: 355.12542724609375\n",
            "Epoch [18000/500000], Loss: 338.545654296875\n",
            "Epoch [19000/500000], Loss: 270.475341796875\n",
            "Epoch [20000/500000], Loss: 220.32211303710938\n",
            "Epoch [21000/500000], Loss: 169.9135284423828\n",
            "Epoch [22000/500000], Loss: 140.05067443847656\n",
            "Epoch [23000/500000], Loss: 114.23213195800781\n",
            "Epoch [24000/500000], Loss: 91.58683776855469\n",
            "Epoch [25000/500000], Loss: 73.9684829711914\n",
            "Epoch [26000/500000], Loss: 60.71515655517578\n",
            "Epoch [27000/500000], Loss: 50.918548583984375\n",
            "Epoch [28000/500000], Loss: 42.699684143066406\n",
            "Epoch [29000/500000], Loss: 35.734737396240234\n",
            "Epoch [30000/500000], Loss: 29.809480667114258\n",
            "Epoch [31000/500000], Loss: 24.28044319152832\n",
            "Epoch [32000/500000], Loss: 19.94276237487793\n",
            "Epoch [33000/500000], Loss: 16.766971588134766\n",
            "Epoch [34000/500000], Loss: 14.183981895446777\n",
            "Epoch [35000/500000], Loss: 11.592496871948242\n",
            "Epoch [36000/500000], Loss: 10.069161415100098\n",
            "Epoch [37000/500000], Loss: 8.898958206176758\n",
            "Epoch [38000/500000], Loss: 7.527580261230469\n",
            "Epoch [39000/500000], Loss: 6.556692123413086\n",
            "Epoch [40000/500000], Loss: 5.738151550292969\n",
            "Epoch [41000/500000], Loss: 5.1310811042785645\n",
            "Epoch [42000/500000], Loss: 4.62033224105835\n",
            "Epoch [43000/500000], Loss: 4.1742658615112305\n",
            "Epoch [44000/500000], Loss: 3.7916815280914307\n",
            "Epoch [45000/500000], Loss: 3.4826815128326416\n",
            "Epoch [46000/500000], Loss: 3.196275234222412\n",
            "Epoch [47000/500000], Loss: 2.9546637535095215\n",
            "Epoch [48000/500000], Loss: 2.7577130794525146\n",
            "Epoch [49000/500000], Loss: 2.570875406265259\n",
            "Epoch [50000/500000], Loss: 2.4330904483795166\n",
            "Epoch [51000/500000], Loss: 2.308865547180176\n",
            "Epoch [52000/500000], Loss: 2.19378662109375\n",
            "Epoch [53000/500000], Loss: 2.110076904296875\n",
            "Epoch [54000/500000], Loss: 1.9989557266235352\n",
            "Epoch [55000/500000], Loss: 1.89496648311615\n",
            "Epoch [56000/500000], Loss: 1.8133509159088135\n",
            "Epoch [57000/500000], Loss: 1.7427871227264404\n",
            "Epoch [58000/500000], Loss: 1.6633046865463257\n",
            "Epoch [59000/500000], Loss: 1.602789282798767\n",
            "Epoch [60000/500000], Loss: 1.551259994506836\n",
            "Epoch [61000/500000], Loss: 1.5058610439300537\n",
            "Epoch [62000/500000], Loss: 1.4570013284683228\n",
            "Epoch [63000/500000], Loss: 1.4150103330612183\n",
            "Epoch [64000/500000], Loss: 1.3958147764205933\n",
            "Epoch [65000/500000], Loss: 1.384503722190857\n",
            "Epoch [66000/500000], Loss: 1.3140876293182373\n",
            "Epoch [67000/500000], Loss: 1.2875953912734985\n",
            "Epoch [68000/500000], Loss: 1.2743037939071655\n",
            "Epoch [69000/500000], Loss: 1.240809440612793\n",
            "Epoch [70000/500000], Loss: 1.2235954999923706\n",
            "Epoch [71000/500000], Loss: 1.2194052934646606\n",
            "Epoch [72000/500000], Loss: 1.195948839187622\n",
            "Epoch [73000/500000], Loss: 1.199897050857544\n",
            "Epoch [74000/500000], Loss: 1.1888458728790283\n",
            "Epoch [75000/500000], Loss: 1.1563583612442017\n",
            "Epoch [76000/500000], Loss: 1.139714241027832\n",
            "Epoch [77000/500000], Loss: 1.128525733947754\n",
            "Epoch [78000/500000], Loss: 1.1171910762786865\n",
            "Epoch [79000/500000], Loss: 1.1045808792114258\n",
            "Epoch [80000/500000], Loss: 1.1041842699050903\n",
            "Epoch [81000/500000], Loss: 1.0910860300064087\n",
            "Epoch [82000/500000], Loss: 1.075756549835205\n",
            "Epoch [83000/500000], Loss: 1.0669708251953125\n",
            "Epoch [84000/500000], Loss: 1.0612549781799316\n",
            "Epoch [85000/500000], Loss: 1.0535792112350464\n",
            "Epoch [86000/500000], Loss: 1.0534435510635376\n",
            "Epoch [87000/500000], Loss: 1.0779728889465332\n",
            "Epoch [88000/500000], Loss: 1.0299729108810425\n",
            "Epoch [89000/500000], Loss: 1.0402239561080933\n",
            "Epoch [90000/500000], Loss: 1.0159204006195068\n",
            "Epoch [91000/500000], Loss: 1.0145761966705322\n",
            "Epoch [92000/500000], Loss: 0.9947807192802429\n",
            "Epoch [93000/500000], Loss: 1.0072394609451294\n",
            "Epoch [94000/500000], Loss: 0.9758463501930237\n",
            "Epoch [95000/500000], Loss: 1.01060950756073\n",
            "Epoch [96000/500000], Loss: 0.9599850177764893\n",
            "Epoch [97000/500000], Loss: 0.9534669518470764\n",
            "Epoch [98000/500000], Loss: 0.9519071578979492\n",
            "Epoch [99000/500000], Loss: 0.9437664747238159\n",
            "Epoch [100000/500000], Loss: 0.948329508304596\n",
            "Epoch [101000/500000], Loss: 0.931303858757019\n",
            "Epoch [102000/500000], Loss: 0.9245215654373169\n",
            "Epoch [103000/500000], Loss: 0.9333118200302124\n",
            "Epoch [104000/500000], Loss: 0.9158185124397278\n",
            "Epoch [105000/500000], Loss: 0.9169257879257202\n",
            "Epoch [106000/500000], Loss: 0.9229282736778259\n",
            "Epoch [107000/500000], Loss: 0.9041574001312256\n",
            "Epoch [108000/500000], Loss: 0.9183949828147888\n",
            "Epoch [109000/500000], Loss: 0.8953791856765747\n",
            "Epoch [110000/500000], Loss: 0.8980194330215454\n",
            "Epoch [111000/500000], Loss: 0.8875913023948669\n",
            "Epoch [112000/500000], Loss: 0.9403180480003357\n",
            "Epoch [113000/500000], Loss: 0.8824504017829895\n",
            "Epoch [114000/500000], Loss: 0.878652036190033\n",
            "Epoch [115000/500000], Loss: 0.8752345442771912\n",
            "Epoch [116000/500000], Loss: 0.8680863380432129\n",
            "Epoch [117000/500000], Loss: 0.872740626335144\n",
            "Epoch [118000/500000], Loss: 0.8619762659072876\n",
            "Epoch [119000/500000], Loss: 0.858422040939331\n",
            "Epoch [120000/500000], Loss: 0.8536062240600586\n",
            "Epoch [121000/500000], Loss: 0.8551759719848633\n",
            "Epoch [122000/500000], Loss: 0.8487692475318909\n",
            "Epoch [123000/500000], Loss: 0.8488827347755432\n",
            "Epoch [124000/500000], Loss: 0.866677463054657\n",
            "Epoch [125000/500000], Loss: 0.8412435054779053\n",
            "Epoch [126000/500000], Loss: 0.8435007333755493\n",
            "Epoch [127000/500000], Loss: 0.8408981561660767\n",
            "Epoch [128000/500000], Loss: 0.8452598452568054\n",
            "Epoch [129000/500000], Loss: 0.8393619060516357\n",
            "Epoch [130000/500000], Loss: 0.8320257067680359\n",
            "Epoch [131000/500000], Loss: 0.8271118402481079\n",
            "Epoch [132000/500000], Loss: 0.8300855755805969\n",
            "Epoch [133000/500000], Loss: 0.8275903463363647\n",
            "Epoch [134000/500000], Loss: 0.819314181804657\n",
            "Epoch [135000/500000], Loss: 0.8285604119300842\n",
            "Epoch [136000/500000], Loss: 0.826184868812561\n",
            "Epoch [137000/500000], Loss: 0.8139315247535706\n",
            "Epoch [138000/500000], Loss: 0.8150850534439087\n",
            "Epoch [139000/500000], Loss: 0.8132365942001343\n",
            "Epoch [140000/500000], Loss: 0.8306259512901306\n",
            "Epoch [141000/500000], Loss: 0.807168185710907\n",
            "Epoch [142000/500000], Loss: 0.8062776923179626\n",
            "Epoch [143000/500000], Loss: 0.8036385774612427\n",
            "Epoch [144000/500000], Loss: 0.8059160113334656\n",
            "Epoch [145000/500000], Loss: 0.801133930683136\n",
            "Epoch [146000/500000], Loss: 0.8001282215118408\n",
            "Epoch [147000/500000], Loss: 0.8013780117034912\n",
            "Epoch [148000/500000], Loss: 0.7951266169548035\n",
            "Epoch [149000/500000], Loss: 0.7963312864303589\n",
            "Epoch [150000/500000], Loss: 0.7914019823074341\n",
            "Epoch [151000/500000], Loss: 0.7951138019561768\n",
            "Epoch [152000/500000], Loss: 0.7888658046722412\n",
            "Epoch [153000/500000], Loss: 0.7872838377952576\n",
            "Epoch [154000/500000], Loss: 0.8156002759933472\n",
            "Epoch [155000/500000], Loss: 0.7834480404853821\n",
            "Epoch [156000/500000], Loss: 0.784020185470581\n",
            "Epoch [157000/500000], Loss: 0.7829684019088745\n",
            "Epoch [158000/500000], Loss: 0.7788410186767578\n",
            "Epoch [159000/500000], Loss: 0.7776955366134644\n",
            "Epoch [160000/500000], Loss: 0.7768658995628357\n",
            "Epoch [161000/500000], Loss: 0.7779346108436584\n",
            "Epoch [162000/500000], Loss: 0.7738320231437683\n",
            "Epoch [163000/500000], Loss: 0.7725353240966797\n",
            "Epoch [164000/500000], Loss: 0.7802214026451111\n",
            "Epoch [165000/500000], Loss: 0.7718449831008911\n",
            "Epoch [166000/500000], Loss: 0.7740054726600647\n",
            "Epoch [167000/500000], Loss: 0.7702400088310242\n",
            "Epoch [168000/500000], Loss: 0.7664034962654114\n",
            "Epoch [169000/500000], Loss: 0.7650807499885559\n",
            "Epoch [170000/500000], Loss: 0.7718319296836853\n",
            "Epoch [171000/500000], Loss: 0.7764222621917725\n",
            "Epoch [172000/500000], Loss: 0.761659562587738\n",
            "Epoch [173000/500000], Loss: 0.7687473893165588\n",
            "Epoch [174000/500000], Loss: 0.7588574886322021\n",
            "Epoch [175000/500000], Loss: 0.769743800163269\n",
            "Epoch [176000/500000], Loss: 0.7577028274536133\n",
            "Epoch [177000/500000], Loss: 0.7555320262908936\n",
            "Epoch [178000/500000], Loss: 0.7573569416999817\n",
            "Epoch [179000/500000], Loss: 0.7564753293991089\n",
            "Epoch [180000/500000], Loss: 0.757182776927948\n",
            "Epoch [181000/500000], Loss: 0.813060998916626\n",
            "Epoch [182000/500000], Loss: 0.7561123967170715\n",
            "Epoch [183000/500000], Loss: 0.7642216086387634\n",
            "Epoch [184000/500000], Loss: 0.7478852868080139\n",
            "Epoch [185000/500000], Loss: 0.7475026249885559\n",
            "Epoch [186000/500000], Loss: 0.7495205998420715\n",
            "Epoch [187000/500000], Loss: 0.7509221434593201\n",
            "Epoch [188000/500000], Loss: 0.7536709308624268\n",
            "Epoch [189000/500000], Loss: 0.7421728372573853\n",
            "Epoch [190000/500000], Loss: 0.7527307271957397\n",
            "Epoch [191000/500000], Loss: 0.7385286688804626\n",
            "Epoch [192000/500000], Loss: 0.7865363955497742\n",
            "Epoch [193000/500000], Loss: 0.7477471828460693\n",
            "Epoch [194000/500000], Loss: 0.7529493570327759\n",
            "Epoch [195000/500000], Loss: 0.7340712547302246\n",
            "Epoch [196000/500000], Loss: 0.7333394885063171\n",
            "Epoch [197000/500000], Loss: 0.7345107793807983\n",
            "Epoch [198000/500000], Loss: 0.7341441512107849\n",
            "Epoch [199000/500000], Loss: 0.7322272062301636\n",
            "Epoch [200000/500000], Loss: 0.7300077676773071\n",
            "Epoch [201000/500000], Loss: 0.730075478553772\n",
            "Epoch [202000/500000], Loss: 0.7283134460449219\n",
            "Epoch [203000/500000], Loss: 0.7294157147407532\n",
            "Epoch [204000/500000], Loss: 0.7276421785354614\n",
            "Epoch [205000/500000], Loss: 0.726219117641449\n",
            "Epoch [206000/500000], Loss: 0.7268599271774292\n",
            "Epoch [207000/500000], Loss: 0.7245569825172424\n",
            "Epoch [208000/500000], Loss: 0.723710298538208\n",
            "Epoch [209000/500000], Loss: 0.7235584259033203\n",
            "Epoch [210000/500000], Loss: 0.7223130464553833\n",
            "Epoch [211000/500000], Loss: 0.7214846014976501\n",
            "Epoch [212000/500000], Loss: 0.7491352558135986\n",
            "Epoch [213000/500000], Loss: 0.7201137542724609\n",
            "Epoch [214000/500000], Loss: 0.71945720911026\n",
            "Epoch [215000/500000], Loss: 0.7753185629844666\n",
            "Epoch [216000/500000], Loss: 0.7182865738868713\n",
            "Epoch [217000/500000], Loss: 0.7791990637779236\n",
            "Epoch [218000/500000], Loss: 0.7170482277870178\n",
            "Epoch [219000/500000], Loss: 0.7165201306343079\n",
            "Epoch [220000/500000], Loss: 0.7158973217010498\n",
            "Epoch [221000/500000], Loss: 0.7758863568305969\n",
            "Epoch [222000/500000], Loss: 0.7150681614875793\n",
            "Epoch [223000/500000], Loss: 0.7137549519538879\n",
            "Epoch [224000/500000], Loss: 0.7133557200431824\n",
            "Epoch [225000/500000], Loss: 0.7098152041435242\n",
            "Epoch [226000/500000], Loss: 0.7096642255783081\n",
            "Epoch [227000/500000], Loss: 0.7078285813331604\n",
            "Epoch [228000/500000], Loss: 0.7234121561050415\n",
            "Epoch [229000/500000], Loss: 0.7062269449234009\n",
            "Epoch [230000/500000], Loss: 0.7058379054069519\n",
            "Epoch [231000/500000], Loss: 0.704937219619751\n",
            "Epoch [232000/500000], Loss: 0.7043452262878418\n",
            "Epoch [233000/500000], Loss: 0.7037508487701416\n",
            "Epoch [234000/500000], Loss: 0.706212043762207\n",
            "Epoch [235000/500000], Loss: 0.7134137749671936\n",
            "Epoch [236000/500000], Loss: 0.7014504075050354\n",
            "Epoch [237000/500000], Loss: 0.7023814916610718\n",
            "Epoch [238000/500000], Loss: 0.6999274492263794\n",
            "Epoch [239000/500000], Loss: 0.700381875038147\n",
            "Epoch [240000/500000], Loss: 0.6996687054634094\n",
            "Epoch [241000/500000], Loss: 0.7012625336647034\n",
            "Epoch [242000/500000], Loss: 0.7040238976478577\n",
            "Epoch [243000/500000], Loss: 0.6971853375434875\n",
            "Epoch [244000/500000], Loss: 0.6966712474822998\n",
            "Epoch [245000/500000], Loss: 0.7029774785041809\n",
            "Epoch [246000/500000], Loss: 0.6958285570144653\n",
            "Epoch [247000/500000], Loss: 0.7326452136039734\n",
            "Epoch [248000/500000], Loss: 0.695309579372406\n",
            "Epoch [249000/500000], Loss: 0.6959182620048523\n",
            "Epoch [250000/500000], Loss: 0.6942564845085144\n",
            "Epoch [251000/500000], Loss: 0.694033682346344\n",
            "Epoch [252000/500000], Loss: 0.6947917342185974\n",
            "Epoch [253000/500000], Loss: 0.725849986076355\n",
            "Epoch [254000/500000], Loss: 0.6928399205207825\n",
            "Epoch [255000/500000], Loss: 0.6931445598602295\n",
            "Epoch [256000/500000], Loss: 0.7009118795394897\n",
            "Epoch [257000/500000], Loss: 0.6950150728225708\n",
            "Epoch [258000/500000], Loss: 0.6919983625411987\n",
            "Epoch [259000/500000], Loss: 0.691325843334198\n",
            "Epoch [260000/500000], Loss: 0.6909107565879822\n",
            "Epoch [261000/500000], Loss: 0.7232063412666321\n",
            "Epoch [262000/500000], Loss: 0.7230780124664307\n",
            "Epoch [263000/500000], Loss: 0.6900073289871216\n",
            "Epoch [264000/500000], Loss: 0.6895797252655029\n",
            "Epoch [265000/500000], Loss: 0.6897468566894531\n",
            "Epoch [266000/500000], Loss: 0.6950404644012451\n",
            "Epoch [267000/500000], Loss: 0.692011296749115\n",
            "Epoch [268000/500000], Loss: 0.6889272928237915\n",
            "Epoch [269000/500000], Loss: 0.6877709031105042\n",
            "Epoch [270000/500000], Loss: 0.6898362040519714\n",
            "Epoch [271000/500000], Loss: 0.6875947713851929\n",
            "Epoch [272000/500000], Loss: 0.686750590801239\n",
            "Epoch [273000/500000], Loss: 0.6868704557418823\n",
            "Epoch [274000/500000], Loss: 0.686360776424408\n",
            "Epoch [275000/500000], Loss: 0.6856512427330017\n",
            "Epoch [276000/500000], Loss: 0.6849197745323181\n",
            "Epoch [277000/500000], Loss: 0.6844637989997864\n",
            "Epoch [278000/500000], Loss: 0.684292197227478\n",
            "Epoch [279000/500000], Loss: 0.7129451036453247\n",
            "Epoch [280000/500000], Loss: 0.6832354664802551\n",
            "Epoch [281000/500000], Loss: 0.6828888058662415\n",
            "Epoch [282000/500000], Loss: 0.6825748085975647\n",
            "Epoch [283000/500000], Loss: 0.6821638941764832\n",
            "Epoch [284000/500000], Loss: 0.6818130612373352\n",
            "Epoch [285000/500000], Loss: 0.7448417544364929\n",
            "Epoch [286000/500000], Loss: 0.6860559582710266\n",
            "Epoch [287000/500000], Loss: 0.6817150712013245\n",
            "Epoch [288000/500000], Loss: 0.6802850961685181\n",
            "Epoch [289000/500000], Loss: 0.7045327425003052\n",
            "Epoch [290000/500000], Loss: 0.6784376502037048\n",
            "Epoch [291000/500000], Loss: 0.6865754723548889\n",
            "Epoch [292000/500000], Loss: 0.6775869727134705\n",
            "Epoch [293000/500000], Loss: 0.6770563721656799\n",
            "Epoch [294000/500000], Loss: 0.8000818490982056\n",
            "Epoch [295000/500000], Loss: 0.6763399243354797\n",
            "Epoch [296000/500000], Loss: 0.6762390732765198\n",
            "Epoch [297000/500000], Loss: 0.6821853518486023\n",
            "Epoch [298000/500000], Loss: 0.6759006381034851\n",
            "Epoch [299000/500000], Loss: 0.6751466393470764\n",
            "Epoch [300000/500000], Loss: 0.6746111512184143\n",
            "Epoch [301000/500000], Loss: 0.6766278743743896\n",
            "Epoch [302000/500000], Loss: 0.675038754940033\n",
            "Epoch [303000/500000], Loss: 0.674525260925293\n",
            "Epoch [304000/500000], Loss: 0.6790737509727478\n",
            "Epoch [305000/500000], Loss: 0.6742121577262878\n",
            "Epoch [306000/500000], Loss: 0.6721286773681641\n",
            "Epoch [307000/500000], Loss: 0.6719187498092651\n",
            "Epoch [308000/500000], Loss: 0.6720354557037354\n",
            "Epoch [309000/500000], Loss: 0.6717561483383179\n",
            "Epoch [310000/500000], Loss: 0.8265039324760437\n",
            "Epoch [311000/500000], Loss: 0.7038008570671082\n",
            "Epoch [312000/500000], Loss: 0.6728363037109375\n",
            "Epoch [313000/500000], Loss: 0.6722561717033386\n",
            "Epoch [314000/500000], Loss: 0.6694862246513367\n",
            "Epoch [315000/500000], Loss: 0.6690996885299683\n",
            "Epoch [316000/500000], Loss: 0.6684328317642212\n",
            "Epoch [317000/500000], Loss: 0.6676714420318604\n",
            "Epoch [318000/500000], Loss: 0.6701453924179077\n",
            "Epoch [319000/500000], Loss: 0.6661303043365479\n",
            "Epoch [320000/500000], Loss: 0.6682037115097046\n",
            "Epoch [321000/500000], Loss: 0.6855608224868774\n",
            "Epoch [322000/500000], Loss: 0.6642709374427795\n",
            "Epoch [323000/500000], Loss: 0.6642144322395325\n",
            "Epoch [324000/500000], Loss: 0.676266610622406\n",
            "Epoch [325000/500000], Loss: 0.6649035811424255\n",
            "Epoch [326000/500000], Loss: 0.6728959679603577\n",
            "Epoch [327000/500000], Loss: 0.6624089479446411\n",
            "Epoch [328000/500000], Loss: 0.6623361110687256\n",
            "Epoch [329000/500000], Loss: 0.6614928245544434\n",
            "Epoch [330000/500000], Loss: 0.6641720533370972\n",
            "Epoch [331000/500000], Loss: 0.6607743501663208\n",
            "Epoch [332000/500000], Loss: 0.6607080101966858\n",
            "Epoch [333000/500000], Loss: 0.6601957082748413\n",
            "Epoch [334000/500000], Loss: 0.6599484086036682\n",
            "Epoch [335000/500000], Loss: 0.6654487252235413\n",
            "Epoch [336000/500000], Loss: 0.6594642400741577\n",
            "Epoch [337000/500000], Loss: 0.6591431498527527\n",
            "Epoch [338000/500000], Loss: 0.6589502096176147\n",
            "Epoch [339000/500000], Loss: 0.7061259746551514\n",
            "Epoch [340000/500000], Loss: 0.6587098836898804\n",
            "Epoch [341000/500000], Loss: 0.6793133020401001\n",
            "Epoch [342000/500000], Loss: 0.6610705256462097\n",
            "Epoch [343000/500000], Loss: 0.6607601642608643\n",
            "Epoch [344000/500000], Loss: 0.6574322581291199\n",
            "Epoch [345000/500000], Loss: 0.6725374460220337\n",
            "Epoch [346000/500000], Loss: 0.6569209098815918\n",
            "Epoch [347000/500000], Loss: 0.6566961407661438\n",
            "Epoch [348000/500000], Loss: 0.6663194298744202\n",
            "Epoch [349000/500000], Loss: 0.6603438258171082\n",
            "Epoch [350000/500000], Loss: 0.6660085916519165\n",
            "Epoch [351000/500000], Loss: 0.6562559008598328\n",
            "Epoch [352000/500000], Loss: 0.65614253282547\n",
            "Epoch [353000/500000], Loss: 0.6561305522918701\n",
            "Epoch [354000/500000], Loss: 0.6722185611724854\n",
            "Epoch [355000/500000], Loss: 0.6580440998077393\n",
            "Epoch [356000/500000], Loss: 0.6544315814971924\n",
            "Epoch [357000/500000], Loss: 0.6680598258972168\n",
            "Epoch [358000/500000], Loss: 0.6535899639129639\n",
            "Epoch [359000/500000], Loss: 0.6640306711196899\n",
            "Epoch [360000/500000], Loss: 0.6529815793037415\n",
            "Epoch [361000/500000], Loss: 0.6527134776115417\n",
            "Epoch [362000/500000], Loss: 0.6710911393165588\n",
            "Epoch [363000/500000], Loss: 0.6522213220596313\n",
            "Epoch [364000/500000], Loss: 0.6576051712036133\n",
            "Epoch [365000/500000], Loss: 0.6537273526191711\n",
            "Epoch [366000/500000], Loss: 0.6538684368133545\n",
            "Epoch [367000/500000], Loss: 0.6510716080665588\n",
            "Epoch [368000/500000], Loss: 0.6538985371589661\n",
            "Epoch [369000/500000], Loss: 0.6510041952133179\n",
            "Epoch [370000/500000], Loss: 0.6562299728393555\n",
            "Epoch [371000/500000], Loss: 0.6643072962760925\n",
            "Epoch [372000/500000], Loss: 0.655200719833374\n",
            "Epoch [373000/500000], Loss: 0.6497740745544434\n",
            "Epoch [374000/500000], Loss: 0.6767072677612305\n",
            "Epoch [375000/500000], Loss: 0.6898823380470276\n",
            "Epoch [376000/500000], Loss: 0.6535360813140869\n",
            "Epoch [377000/500000], Loss: 0.6586183905601501\n",
            "Epoch [378000/500000], Loss: 0.6489050388336182\n",
            "Epoch [379000/500000], Loss: 0.6485830545425415\n",
            "Epoch [380000/500000], Loss: 0.6489529013633728\n",
            "Epoch [381000/500000], Loss: 0.6620539426803589\n",
            "Epoch [382000/500000], Loss: 0.6890017986297607\n",
            "Epoch [383000/500000], Loss: 0.6532661318778992\n",
            "Epoch [384000/500000], Loss: 0.6477383971214294\n",
            "Epoch [385000/500000], Loss: 0.6481690406799316\n",
            "Epoch [386000/500000], Loss: 0.6486571431159973\n",
            "Epoch [387000/500000], Loss: 0.6468800902366638\n",
            "Epoch [388000/500000], Loss: 0.6467094421386719\n",
            "Epoch [389000/500000], Loss: 0.6465588212013245\n",
            "Epoch [390000/500000], Loss: 0.646873950958252\n",
            "Epoch [391000/500000], Loss: 0.7289878129959106\n",
            "Epoch [392000/500000], Loss: 0.6461905837059021\n",
            "Epoch [393000/500000], Loss: 0.6469182372093201\n",
            "Epoch [394000/500000], Loss: 0.6493592262268066\n",
            "Epoch [395000/500000], Loss: 0.6455002427101135\n",
            "Epoch [396000/500000], Loss: 0.6454368233680725\n",
            "Epoch [397000/500000], Loss: 0.6451947689056396\n",
            "Epoch [398000/500000], Loss: 0.6519140601158142\n",
            "Epoch [399000/500000], Loss: 0.6508112549781799\n",
            "Epoch [400000/500000], Loss: 0.6449171304702759\n",
            "Epoch [401000/500000], Loss: 0.6507917642593384\n",
            "Epoch [402000/500000], Loss: 0.6444279551506042\n",
            "Epoch [403000/500000], Loss: 0.6443344354629517\n",
            "Epoch [404000/500000], Loss: 0.6452610492706299\n",
            "Epoch [405000/500000], Loss: 0.643932044506073\n",
            "Epoch [406000/500000], Loss: 0.6439711451530457\n",
            "Epoch [407000/500000], Loss: 0.7183658480644226\n",
            "Epoch [408000/500000], Loss: 0.6453477740287781\n",
            "Epoch [409000/500000], Loss: 0.6440545320510864\n",
            "Epoch [410000/500000], Loss: 0.7451866865158081\n",
            "Epoch [411000/500000], Loss: 0.6430612206459045\n",
            "Epoch [412000/500000], Loss: 0.6429218649864197\n",
            "Epoch [413000/500000], Loss: 0.6432300209999084\n",
            "Epoch [414000/500000], Loss: 0.6438218355178833\n",
            "Epoch [415000/500000], Loss: 0.6438634991645813\n",
            "Epoch [416000/500000], Loss: 0.6423642635345459\n",
            "Epoch [417000/500000], Loss: 0.642233669757843\n",
            "Epoch [418000/500000], Loss: 0.642231822013855\n",
            "Epoch [419000/500000], Loss: 0.6420970559120178\n",
            "Epoch [420000/500000], Loss: 0.6525581479072571\n",
            "Epoch [421000/500000], Loss: 0.6414880156517029\n",
            "Epoch [422000/500000], Loss: 0.6415064334869385\n",
            "Epoch [423000/500000], Loss: 0.6458496451377869\n",
            "Epoch [424000/500000], Loss: 0.6419050693511963\n",
            "Epoch [425000/500000], Loss: 0.640925407409668\n",
            "Epoch [426000/500000], Loss: 0.6401006579399109\n",
            "Epoch [427000/500000], Loss: 0.6407217979431152\n",
            "Epoch [428000/500000], Loss: 0.6396466493606567\n",
            "Epoch [429000/500000], Loss: 0.6418577432632446\n",
            "Epoch [430000/500000], Loss: 0.6395994424819946\n",
            "Epoch [431000/500000], Loss: 0.6390545964241028\n",
            "Epoch [432000/500000], Loss: 0.6394798755645752\n",
            "Epoch [433000/500000], Loss: 0.6383686065673828\n",
            "Epoch [434000/500000], Loss: 0.6651308536529541\n",
            "Epoch [435000/500000], Loss: 0.6381884813308716\n",
            "Epoch [436000/500000], Loss: 0.6378055214881897\n",
            "Epoch [437000/500000], Loss: 0.6374621987342834\n",
            "Epoch [438000/500000], Loss: 0.6376462578773499\n",
            "Epoch [439000/500000], Loss: 0.6379680037498474\n",
            "Epoch [440000/500000], Loss: 0.6415462493896484\n",
            "Epoch [441000/500000], Loss: 0.6633009314537048\n",
            "Epoch [442000/500000], Loss: 0.6511229872703552\n",
            "Epoch [443000/500000], Loss: 0.6446234583854675\n",
            "Epoch [444000/500000], Loss: 0.6367243528366089\n",
            "Epoch [445000/500000], Loss: 0.636138916015625\n",
            "Epoch [446000/500000], Loss: 0.6360445618629456\n",
            "Epoch [447000/500000], Loss: 0.6389750242233276\n",
            "Epoch [448000/500000], Loss: 0.6374394297599792\n",
            "Epoch [449000/500000], Loss: 0.6355984807014465\n",
            "Epoch [450000/500000], Loss: 0.6357461214065552\n",
            "Epoch [451000/500000], Loss: 0.6453051567077637\n",
            "Epoch [452000/500000], Loss: 0.6366738677024841\n",
            "Epoch [453000/500000], Loss: 0.6384244561195374\n",
            "Epoch [454000/500000], Loss: 0.6348550319671631\n",
            "Epoch [455000/500000], Loss: 0.6359149813652039\n",
            "Epoch [456000/500000], Loss: 0.634754478931427\n",
            "Epoch [457000/500000], Loss: 0.642685055732727\n",
            "Epoch [458000/500000], Loss: 0.6870558857917786\n",
            "Epoch [459000/500000], Loss: 0.634093165397644\n",
            "Epoch [460000/500000], Loss: 0.6340116262435913\n",
            "Epoch [461000/500000], Loss: 0.6341817378997803\n",
            "Epoch [462000/500000], Loss: 0.6337296366691589\n",
            "Epoch [463000/500000], Loss: 0.642892062664032\n",
            "Epoch [464000/500000], Loss: 0.6670240759849548\n",
            "Epoch [465000/500000], Loss: 0.633741021156311\n",
            "Epoch [466000/500000], Loss: 0.6566140055656433\n",
            "Epoch [467000/500000], Loss: 0.6522495150566101\n",
            "Epoch [468000/500000], Loss: 0.6635640859603882\n",
            "Epoch [469000/500000], Loss: 0.7270146608352661\n",
            "Epoch [470000/500000], Loss: 0.632954478263855\n",
            "Epoch [471000/500000], Loss: 0.6327883005142212\n",
            "Epoch [472000/500000], Loss: 0.6356284022331238\n",
            "Epoch [473000/500000], Loss: 0.6993468403816223\n",
            "Epoch [474000/500000], Loss: 0.6325461268424988\n",
            "Epoch [475000/500000], Loss: 0.6321148872375488\n",
            "Epoch [476000/500000], Loss: 0.6320064663887024\n",
            "Epoch [477000/500000], Loss: 0.6319451332092285\n",
            "Epoch [478000/500000], Loss: 0.6319337487220764\n",
            "Epoch [479000/500000], Loss: 0.6465234160423279\n",
            "Epoch [480000/500000], Loss: 0.631759524345398\n",
            "Epoch [481000/500000], Loss: 0.6327088475227356\n",
            "Epoch [482000/500000], Loss: 0.6342471241950989\n",
            "Epoch [483000/500000], Loss: 0.648402988910675\n",
            "Epoch [484000/500000], Loss: 0.6314358115196228\n",
            "Epoch [485000/500000], Loss: 0.6309799551963806\n",
            "Epoch [486000/500000], Loss: 0.6315985918045044\n",
            "Epoch [487000/500000], Loss: 0.6311998963356018\n",
            "Epoch [488000/500000], Loss: 0.6307753920555115\n",
            "Epoch [489000/500000], Loss: 0.6511564254760742\n",
            "Epoch [490000/500000], Loss: 0.6326712965965271\n",
            "Epoch [491000/500000], Loss: 0.6725084185600281\n",
            "Epoch [492000/500000], Loss: 0.6415905952453613\n",
            "Epoch [493000/500000], Loss: 0.6308937072753906\n",
            "Epoch [494000/500000], Loss: 0.63106769323349\n",
            "Epoch [495000/500000], Loss: 0.6304211020469666\n",
            "Epoch [496000/500000], Loss: 0.6307995915412903\n",
            "Epoch [497000/500000], Loss: 0.6306668519973755\n",
            "Epoch [498000/500000], Loss: 0.629896342754364\n",
            "Epoch [499000/500000], Loss: 0.6346339583396912\n",
            "Epoch [500000/500000], Loss: 0.6294960975646973\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained model to make predictions\n",
        "new_input = torch.tensor([[0.400, -48]], dtype=torch.float32)\n",
        "\n",
        "predicted_imd3_distortion = model(new_input).item()\n",
        "print(f\"Predicted IMD3 Distortion: {predicted_imd3_distortion}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zLb8QoQEKCb",
        "outputId": "db9515cd-429d-40f5-c5a1-5828672af224"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted IMD3 Distortion: -335.9605712890625\n"
          ]
        }
      ]
    }
  ]
}